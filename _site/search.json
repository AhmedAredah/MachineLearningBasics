[
  {
    "objectID": "posts/2.Regression/NumericalDataPrediction.html",
    "href": "posts/2.Regression/NumericalDataPrediction.html",
    "title": "Multiple Linear Regression: Application on FARS Dataset",
    "section": "",
    "text": "When modeling the relationship between a dependent variable y and multiple independent variables x_1, x_2, \\dots, x_n, the linear equation can be written as: \\begin{equation}\ny_i=\\beta_0+\\beta_1 x_{i 1}+\\beta_2 x_{i 2}+\\cdots+\\beta_n x_{i n}+\\epsilon_i\n\\end{equation}\nwhere:\n\ny_i is the observed value of the dependent variable for the i^{th} observation.\nx_1, x_2, \\dots, x_n are the values of the independent variables for the i^{th} observation.\n\\beta_0, \\beta_1, \\beta_n are the regression coefficients, with \\beta_0 being the y-intercept.\n\\epsilon_i is the error term for the i^{th} observation, capturing the difference between the observed value and the value predicted by the model.\n\nThe requirement here is to find the \\beta_i that minimizes the meas square error of \\epsilon_i. The above could be solved in multiple ways. However, one easy way to solve it is through matrix multiplication since CPU’s can deal faster with matrix manipulation.\n\n\nGiven:\n\n\\boldsymbol{X} is the design matrix of size m \\times (n + 1).\n\\boldsymbol{y} is a column vector of size m \\times 1 containing the dependent variable values.\n\\boldsymbol{\\beta} is a column vector of size (n + 1) \\times 1 containing the regression coefficients.\n\\boldsymbol{\\epsilon} is a column vector of size m \\times 1 representing the errors.\n\nThe relationship is given by: \\begin{equation}\n\\large{y} = \\large{X} \\beta + \\epsilon\n\\end{equation}\nTo determine the value of \\beta, one might think to rearrange the equation as:\n\\begin{equation}\n\\beta = \\frac{\\large{y}}{\\large{X}}\n\\end{equation}\nHowever, this representation is not accurate in the context of matrix operations. The inaccuracy arises due to the nature of matrices and how they are manipulated. Here’s a breakdown of the issues:\n\nMatrix Division: In the realm of matrices, there isn’t a direct concept of division like there is with regular numbers. So, saying \\beta = \\frac{y}{X} doesn’t have a straightforward meaning.\nMatrix Multiplication: Matrix multiplication is not commutative. This means that the product AB is not necessarily the same as the product BA. So, even if we were to try to “isolate” \\beta by some matrix operation, it wouldn’t be as simple as dividing both sides by X.\nCorrect Approach: The correct way to “isolate” \\beta when dealing with matrices is to multiply both sides of the equation by the inverse of X (assuming X is invertible). The equation would look something like: \\beta = X^{-1}y. Note that this equation assumes that X is a square matrix and has an inverse. If X is not square, or doesn’t have an inverse, other methods like the Moore-Penrose pseudoinverse would be used to estimate \\beta.\nDimensionality: Even if we were to entertain the idea of matrix division, the dimensions must be compatible. In the equation y = X\\beta + \\epsilon, y is a column vector of size m \\times 1, X is a matrix of size m \\times n, and \\beta is a column vector of size n \\times 1. Dividing an m \\times 1 vector by an m \\times n matrix doesn’t produce a consistent result in terms of matrix dimensions.\n\nThe reason we use the equation $ = (^T )^{-1} ^T $ instead of $ = ^{-1} $ is due to the structure and properties of the design matrix ( ) in linear regression.\n\nNon-Square Matrix: In most real-world applications of linear regression, \\mathbf{X} is not a square matrix. It usually has more rows (observations) than columns (predictors). Only square matrices possess inverses in the traditional sense. Therefore, \\mathbf{X}^{-1} doesn’t exist for these cases.\nPseudo-Inverse: The expression (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T is known as the Moore-Penrose pseudo-inverse of \\mathbf{X}. This pseudo-inverse provides a means to approximate an inverse.\nProjection onto Column Space: The term \\mathbf{X}^T \\mathbf{y} can be interpreted as projecting the response vector \\mathbf{y} onto the column space of \\mathbf{X}.\nMinimization of Residuals: The expression (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T originates from differentiating the sum of squared residuals with respect to \\boldsymbol{\\beta} and setting it to zero.\n\nWe’ll be applying this concept to the FARS dataset. Managed by the U.S. National Highway Traffic Safety Administration (NHTSA), the Fatality Analysis Reporting System (FARS) provides an extensive record of fatal motor vehicle crashes in the U.S. and Puerto Rico since 1975."
  },
  {
    "objectID": "posts/2.Regression/NumericalDataPrediction.html#mathematical-formulation",
    "href": "posts/2.Regression/NumericalDataPrediction.html#mathematical-formulation",
    "title": "Multiple Linear Regression: Application on FARS Dataset",
    "section": "",
    "text": "When modeling the relationship between a dependent variable y and multiple independent variables x_1, x_2, \\dots, x_n, the linear equation can be written as: \\begin{equation}\ny_i=\\beta_0+\\beta_1 x_{i 1}+\\beta_2 x_{i 2}+\\cdots+\\beta_n x_{i n}+\\epsilon_i\n\\end{equation}\nwhere:\n\ny_i is the observed value of the dependent variable for the i^{th} observation.\nx_1, x_2, \\dots, x_n are the values of the independent variables for the i^{th} observation.\n\\beta_0, \\beta_1, \\beta_n are the regression coefficients, with \\beta_0 being the y-intercept.\n\\epsilon_i is the error term for the i^{th} observation, capturing the difference between the observed value and the value predicted by the model.\n\nThe requirement here is to find the \\beta_i that minimizes the meas square error of \\epsilon_i. The above could be solved in multiple ways. However, one easy way to solve it is through matrix multiplication since CPU’s can deal faster with matrix manipulation.\n\n\nGiven:\n\n\\boldsymbol{X} is the design matrix of size m \\times (n + 1).\n\\boldsymbol{y} is a column vector of size m \\times 1 containing the dependent variable values.\n\\boldsymbol{\\beta} is a column vector of size (n + 1) \\times 1 containing the regression coefficients.\n\\boldsymbol{\\epsilon} is a column vector of size m \\times 1 representing the errors.\n\nThe relationship is given by: \\begin{equation}\n\\large{y} = \\large{X} \\beta + \\epsilon\n\\end{equation}\nTo determine the value of \\beta, one might think to rearrange the equation as:\n\\begin{equation}\n\\beta = \\frac{\\large{y}}{\\large{X}}\n\\end{equation}\nHowever, this representation is not accurate in the context of matrix operations. The inaccuracy arises due to the nature of matrices and how they are manipulated. Here’s a breakdown of the issues:\n\nMatrix Division: In the realm of matrices, there isn’t a direct concept of division like there is with regular numbers. So, saying \\beta = \\frac{y}{X} doesn’t have a straightforward meaning.\nMatrix Multiplication: Matrix multiplication is not commutative. This means that the product AB is not necessarily the same as the product BA. So, even if we were to try to “isolate” \\beta by some matrix operation, it wouldn’t be as simple as dividing both sides by X.\nCorrect Approach: The correct way to “isolate” \\beta when dealing with matrices is to multiply both sides of the equation by the inverse of X (assuming X is invertible). The equation would look something like: \\beta = X^{-1}y. Note that this equation assumes that X is a square matrix and has an inverse. If X is not square, or doesn’t have an inverse, other methods like the Moore-Penrose pseudoinverse would be used to estimate \\beta.\nDimensionality: Even if we were to entertain the idea of matrix division, the dimensions must be compatible. In the equation y = X\\beta + \\epsilon, y is a column vector of size m \\times 1, X is a matrix of size m \\times n, and \\beta is a column vector of size n \\times 1. Dividing an m \\times 1 vector by an m \\times n matrix doesn’t produce a consistent result in terms of matrix dimensions.\n\nThe reason we use the equation $ = (^T )^{-1} ^T $ instead of $ = ^{-1} $ is due to the structure and properties of the design matrix ( ) in linear regression.\n\nNon-Square Matrix: In most real-world applications of linear regression, \\mathbf{X} is not a square matrix. It usually has more rows (observations) than columns (predictors). Only square matrices possess inverses in the traditional sense. Therefore, \\mathbf{X}^{-1} doesn’t exist for these cases.\nPseudo-Inverse: The expression (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T is known as the Moore-Penrose pseudo-inverse of \\mathbf{X}. This pseudo-inverse provides a means to approximate an inverse.\nProjection onto Column Space: The term \\mathbf{X}^T \\mathbf{y} can be interpreted as projecting the response vector \\mathbf{y} onto the column space of \\mathbf{X}.\nMinimization of Residuals: The expression (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T originates from differentiating the sum of squared residuals with respect to \\boldsymbol{\\beta} and setting it to zero.\n\nWe’ll be applying this concept to the FARS dataset. Managed by the U.S. National Highway Traffic Safety Administration (NHTSA), the Fatality Analysis Reporting System (FARS) provides an extensive record of fatal motor vehicle crashes in the U.S. and Puerto Rico since 1975."
  },
  {
    "objectID": "posts/2.Regression/NumericalDataPrediction.html#code",
    "href": "posts/2.Regression/NumericalDataPrediction.html#code",
    "title": "Multiple Linear Regression: Application on FARS Dataset",
    "section": "Code",
    "text": "Code\n\n#import required libraries\nimport pandas as pd\nimport os\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#inline plots\n%matplotlib inline\n\n# Set Seaborn style to \"whitegrid\" for a white background with grid lines\nsns.set_style(\"whitegrid\")\n\n#supress warnings!\nwarnings.simplefilter(action='ignore', category=Warning)\n# Set the display option to show all columns\npd.set_option('display.max_columns', None)\n\nprint(\"required libraries loaded successfully!\")\n\nrequired libraries loaded successfully!\n\n\nThe dataset source is accessible from here\n\n# Load the cars dataset\ncars = pd.read_csv(\"https://raw.githubusercontent.com/AhmedAredah/MachineLearningBasics/main/data/cars.csv\")\n\ncars.describe()\n\n\n\n\n\n\n\n\nyear\nselling_price\nkm_driven\nseats\n\n\n\n\ncount\n8128.000000\n8.128000e+03\n8.128000e+03\n7907.000000\n\n\nmean\n2013.804011\n6.382718e+05\n6.981951e+04\n5.416719\n\n\nstd\n4.044249\n8.062534e+05\n5.655055e+04\n0.959588\n\n\nmin\n1983.000000\n2.999900e+04\n1.000000e+00\n2.000000\n\n\n25%\n2011.000000\n2.549990e+05\n3.500000e+04\n5.000000\n\n\n50%\n2015.000000\n4.500000e+05\n6.000000e+04\n5.000000\n\n\n75%\n2017.000000\n6.750000e+05\n9.800000e+04\n5.000000\n\n\nmax\n2020.000000\n1.000000e+07\n2.360457e+06\n14.000000\n\n\n\n\n\n\n\nThe describe method in pandas provides a summary of the central tendency, dispersion, and shape of the distribution of a dataset. It returns a DataFrame that shows various descriptive statistics.\nHere’s what each row in the output means:\n\ncount: The number of non-missing values for each variable. In this case, each variable has 342 non-missing values.\nmean: The average value of each variable.\nstd: The standard deviation of each variable, which measures the amount of variation or dispersion.\nmin: The minimum value of each variable.\n25%: The 25th percentile value of each variable.\n50%: The 50th percentile value (or median) of each variable.\n75%: The 75th percentile value of each variable.\nmax: The maximum value of each variable.\n\nIn the context of linear regression, these descriptive statistics can help you understand the distribution of your variables and guide your data preprocessing steps.\nOne common issue to address during data preprocessing is the handling of missing values (NA values). There are several ways to deal with missing values:\n\nRemove rows with missing values: This is the simplest approach, but it may result in loss of valuable data. python     data = data.dropna()\nReplace missing values: You can replace missing values with a specific value, such as the mean or median of the variable. python     data['variable'].fillna(data['variable'].mean(), inplace=True)\nUse predictive imputation: This involves using other variables in the dataset to predict and fill in missing values. This can be done using machine learning algorithms or other statistical methods.\n\nBefore fitting a linear regression model, it’s important to check for outliers, as they can have a significant impact on your model. The min and max values in the describe output can help you identify any extreme values that might be outliers. You may also want to plot your data to visually inspect for outliers.\nAdditionally, the mean and std values can be used to standardize your variables, which is a common preprocessing step for linear regression. Standardizing your variables can make it easier to interpret the coefficients of your linear regression model, especially when your variables are on different scales.\nIn this case, We will just remove rows with missing values since it is easier but this could have a huge impact on the dataset.\n\n# Drop rows where any cell contains NA or NAN \ncars = cars.dropna()\n\n# show the top 5 rows\ncars.head()\n\n\n\n\n\n\n\n\nname\nyear\nselling_price\nkm_driven\nfuel\nseller_type\ntransmission\nowner\nmileage\nengine\nmax_power\ntorque\nseats\n\n\n\n\n0\nMaruti Swift Dzire VDI\n2014\n450000\n145500\nDiesel\nIndividual\nManual\nFirst Owner\n23.4 kmpl\n1248 CC\n74 bhp\n190Nm@ 2000rpm\n5.0\n\n\n1\nSkoda Rapid 1.5 TDI Ambition\n2014\n370000\n120000\nDiesel\nIndividual\nManual\nSecond Owner\n21.14 kmpl\n1498 CC\n103.52 bhp\n250Nm@ 1500-2500rpm\n5.0\n\n\n2\nHonda City 2017-2020 EXi\n2006\n158000\n140000\nPetrol\nIndividual\nManual\nThird Owner\n17.7 kmpl\n1497 CC\n78 bhp\n12.7@ 2,700(kgm@ rpm)\n5.0\n\n\n3\nHyundai i20 Sportz Diesel\n2010\n225000\n127000\nDiesel\nIndividual\nManual\nFirst Owner\n23.0 kmpl\n1396 CC\n90 bhp\n22.4 kgm at 1750-2750rpm\n5.0\n\n\n4\nMaruti Swift VXI BSIII\n2007\n130000\n120000\nPetrol\nIndividual\nManual\nFirst Owner\n16.1 kmpl\n1298 CC\n88.2 bhp\n11.5@ 4,500(kgm@ rpm)\n5.0\n\n\n\n\n\n\n\nFrom the above table we need to clean the dataset first and remove strings from columns ‘mileages’, ‘engine’, ‘max_power’, and ‘torque’\n\n# List of columns to process\ncolumns_to_process = ['mileage', 'engine', 'max_power']\n\nfor column in columns_to_process:\n    # Convert the column to string type\n    cars[column] = cars[column].astype(str)\n    \n    # Extract the first numerical value (assumes format is \"value unit\")\n    cars[column] = cars[column].str.split().str[0]\n    \n    # Convert those values to float, set others to NaN if they can't be converted\n    cars[column] = pd.to_numeric(cars[column], errors='coerce')\n\n\nimport re\n\n# Function to extract the numeric part before 'Nm'\ndef extract_torque_value(s):\n    match = re.search(r'(\\d+)Nm', s)\n    return float(match.group(1)) if match else None\n\n# Apply the function to the torque column\ncars['torque'] = cars['torque'].apply(extract_torque_value)\ncars.dropna()\n\n\n\n\n\n\n\n\nname\nyear\nselling_price\nkm_driven\nfuel\nseller_type\ntransmission\nowner\nmileage\nengine\nmax_power\ntorque\nseats\n\n\n\n\n0\nMaruti Swift Dzire VDI\n2014\n450000\n145500\nDiesel\nIndividual\nManual\nFirst Owner\n23.40\n1248\n74.00\n190.0\n5.0\n\n\n1\nSkoda Rapid 1.5 TDI Ambition\n2014\n370000\n120000\nDiesel\nIndividual\nManual\nSecond Owner\n21.14\n1498\n103.52\n250.0\n5.0\n\n\n7\nMaruti 800 DX BSII\n2001\n45000\n5000\nPetrol\nIndividual\nManual\nSecond Owner\n16.10\n796\n37.00\n59.0\n4.0\n\n\n8\nToyota Etios VXD\n2011\n350000\n90000\nDiesel\nIndividual\nManual\nFirst Owner\n23.59\n1364\n67.10\n170.0\n5.0\n\n\n9\nFord Figo Diesel Celebration Edition\n2013\n200000\n169000\nDiesel\nIndividual\nManual\nFirst Owner\n20.00\n1399\n68.10\n160.0\n5.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8122\nHyundai i20 Magna 1.4 CRDi\n2014\n475000\n80000\nDiesel\nIndividual\nManual\nSecond Owner\n22.54\n1396\n88.73\n7.0\n5.0\n\n\n8123\nHyundai i20 Magna\n2013\n320000\n110000\nPetrol\nIndividual\nManual\nFirst Owner\n18.50\n1197\n82.85\n7.0\n5.0\n\n\n8125\nMaruti Swift Dzire ZDi\n2009\n382000\n120000\nDiesel\nIndividual\nManual\nFirst Owner\n19.30\n1248\n73.90\n190.0\n5.0\n\n\n8126\nTata Indigo CR4\n2013\n290000\n25000\nDiesel\nIndividual\nManual\nFirst Owner\n23.57\n1396\n70.00\n140.0\n5.0\n\n\n8127\nTata Indigo CR4\n2013\n290000\n25000\nDiesel\nIndividual\nManual\nFirst Owner\n23.57\n1396\n70.00\n140.0\n5.0\n\n\n\n\n7033 rows × 13 columns\n\n\n\nBefore diving into machine learning, understanding the descriptive statistics of our data is crucial. It provides insights into the distribution, tendencies, and range of our data. This preliminary step ensures that we’re aware of the data’s characteristics, helping in making informed decisions about preprocessing, model selection, and interpretation of results. Such an understanding can aid in identifying anomalies, ensuring data quality, and setting the right expectations from the model’s predictions.\n\ncars.describe()\n\n\n\n\n\n\n\n\nyear\nselling_price\nkm_driven\nmileage\nengine\nmax_power\ntorque\nseats\n\n\n\n\ncount\n7906.000000\n7.906000e+03\n7.906000e+03\n7906.000000\n7906.000000\n7906.000000\n7033.000000\n7906.000000\n\n\nmean\n2013.983936\n6.498137e+05\n6.918866e+04\n19.419861\n1458.708829\n91.587374\n158.266032\n5.416393\n\n\nstd\n3.863695\n8.135827e+05\n5.679230e+04\n4.036263\n503.893057\n35.747216\n107.169575\n0.959208\n\n\nmin\n1994.000000\n2.999900e+04\n1.000000e+00\n0.000000\n624.000000\n32.800000\n1.000000\n2.000000\n\n\n25%\n2012.000000\n2.700000e+05\n3.500000e+04\n16.780000\n1197.000000\n68.050000\n90.000000\n5.000000\n\n\n50%\n2015.000000\n4.500000e+05\n6.000000e+04\n19.300000\n1248.000000\n82.000000\n146.000000\n5.000000\n\n\n75%\n2017.000000\n6.900000e+05\n9.542500e+04\n22.320000\n1582.000000\n102.000000\n200.000000\n5.000000\n\n\nmax\n2020.000000\n1.000000e+07\n2.360457e+06\n42.000000\n3604.000000\n400.000000\n789.000000\n14.000000\n\n\n\n\n\n\n\n\n# Set up the matplotlib figure\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\n\n# Plot the distribution of each variable\nhist1 = sns.histplot(data=cars, x='year', kde=True, ax=axes[0, 0], color='blue')\nhist2 = sns.histplot(data=cars, x='selling_price', kde=True, ax=axes[0, 1], color='blue')\nhist3 = sns.histplot(data=cars, x='km_driven', kde=True, ax=axes[0, 2], color='blue')\nhist4 = sns.histplot(data=cars, x='engine', kde=True, ax=axes[0, 3], color='blue')\nhist5 = sns.histplot(data=cars, x='max_power', kde=True, ax=axes[1, 0], color='blue')\nhist6 = sns.histplot(data=cars, x='mileage', kde=True, ax=axes[1, 1], color='blue')\nhist7 = sns.histplot(data=cars, x='torque', kde=True, ax=axes[1, 2], color='blue')\n\n# Plot the count of each category for categorical variables\nseats_plot = sns.countplot(data=cars, x='seats', ax=axes[1, 3])\nfuel_plot = sns.countplot(data=cars, x='fuel', ax=axes[2, 0])\ntransmission_plot = sns.countplot(data=cars, x='transmission', ax=axes[2, 1])\nowner_plot = sns.countplot(data=cars, x='owner', ax=axes[2, 2])\nseller_plot = sns.countplot(data=cars, x='seller_type', ax=axes[2, 3])\n\n# Make the x-axis text vertical for all plots\nfor ax in axes.flatten():\n    plt.sca(ax)\n    plt.xticks(rotation=45)\n\n# Adjust the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\nWhen applying linear regression to this dataset, it’s crucial to consider these distributions. The insights derived from these plots can help in feature selection, outlier detection, and in understanding the relationships between variables. For instance, the dominance of diesel and petrol cars might mean that other fuel types have less influence on the selling price. Similarly, the large number of manual transmission cars might imply that automatic transmission could be a premium feature, potentially impacting the price.\nfrom the above distribution we find that selling price, and km_driven are positively skewed and we need to apply a transformation function to make it normally distributed.\n\nimport numpy as np\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\ntransformed_data = np.log1p(cars['selling_price'])  # log1p helps in dealing with zero values in the original data\ntransformed_data_2 = np.log1p(cars['km_driven'])\nsns.histplot(transformed_data, ax=axes[0]);\nsns.histplot(transformed_data_2, ax=axes[1]);\n\n\n\n\nWe will compare the regression model with and without the transformation\n\n# Calculate VIF for each predictor variable\ncars_numeric = cars.select_dtypes(include='number')\ncars_numeric = cars_numeric.fillna(cars_numeric.mean())\n\nvif_data = pd.DataFrame()\nvif_data[\"variable\"] = cars_numeric.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(cars_numeric.values, i) for i in range(cars_numeric.shape[1])]\nvif_data.head(n=10)\n\n\n\n\n\n\n\n\nvariable\nVIF\n\n\n\n\n0\nyear\n140.187417\n\n\n1\nselling_price\n4.337223\n\n\n2\nkm_driven\n2.993602\n\n\n3\nmileage\n40.354886\n\n\n4\nengine\n41.552858\n\n\n5\nmax_power\n34.167509\n\n\n6\ntorque\n7.961410\n\n\n7\nseats\n66.993390\n\n\n\n\n\n\n\nVariance Inflation Factor (VIF) is a measure that helps to identify multicollinearity in regression models. When interpreting the VIF, a general rule of thumb is that a VIF above 5-10 suggests a problematic amount of collinearity. Given the high VIF values for ‘Year’, ‘mileage’, ‘engine’, ‘max_power’, and ‘Seats’, one should consider further analysis or remedial measures to address potential multicollinearity before proceeding with building a regression model.\nFrom this, I will drop the highest 2 VIF column value and redo the analysis again.\n\ncars_numeric = cars_numeric.drop([\"year\", \"seats\", \"max_power\"], axis=1)\n\n\nvif_data = pd.DataFrame()\nvif_data[\"variable\"] = cars_numeric.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(cars_numeric.values, i) for i in range(cars_numeric.shape[1])]\nvif_data.head(n=10)\n\n\n\n\n\n\n\n\nvariable\nVIF\n\n\n\n\n0\nselling_price\n2.776218\n\n\n1\nkm_driven\n2.982095\n\n\n2\nmileage\n4.839869\n\n\n3\nengine\n12.135908\n\n\n4\ntorque\n7.028583\n\n\n\n\n\n\n\nThe resultant VIF indicates almost no multicolinearity in the data.\nFor the categorical data, the regression model does not understand strings (text) so we need to find a way to transfer this text in numbers. one way is to encode them which converts the categories into a series of 1,2,3 coresponding to the categorical order. OneHoteEncder does this job for us.\n\ncars = cars.drop([\"year\", \"seats\", \"max_power\"], axis=1)\n\n\n# Preprocessing: One-hot encode categorical variables\ncategorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']\none_hot = OneHotEncoder(drop='first', sparse=False)  # drop='first' to avoid multicollinearity\nencoded_features = one_hot.fit_transform(cars[categorical_columns])\nencoded_df = pd.DataFrame(encoded_features, columns=one_hot.get_feature_names_out(categorical_columns))\n\nBefore we proceed, let’s drop the columns that we decided to drop before in the VIF analysis.\n\n# Concatenate encoded features with the original dataframe\ncars = pd.concat([cars, encoded_df], axis=1)\n\n# Drop the original categorical columns and other non-numeric columns\ncars = cars.drop(columns=categorical_columns + ['name', 'torque'])\n\nAgain, let’s make sure we dont have any missing values before we continue by filling with the mean value.\n\n# Define predictors (X) and target variable (y)\nX = cars.drop('selling_price', axis=1)\ny = cars['selling_price']\nX = X.fillna(X.mean())\ny = y.fillna(y.mean())\n\nWe will do the regression model here.\n\nFirst regression model without transformation\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the Linear Regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = lin_reg.predict(X_test)\n\n# Evaluate the model's performance\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 (coefficient of determination): {r2:.2f}\")\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse:.2f}\")\n\n# Model's coefficients and intercept\nprint(f\"Intercept: {lin_reg.intercept_}\")\nprint(f\"Coefficients: {lin_reg.coef_}\")\n\nR^2 (coefficient of determination): 0.37\nMean Squared Error: 419706652720.62\nIntercept: -960727.9684812507\nCoefficients: [-4.21035185e+00  3.33345338e+04  9.85201248e+02 -1.36989350e+05\n -2.04315878e+05 -1.37114189e+05 -4.11308191e+04  1.61218399e+05\n -7.12835842e+03 -1.31152299e+05 -2.25614271e+04  9.90031360e+05\n -3.43569635e+04]\n\n\n\n# Selecting the first feature for demonstration\nfeature_index = 0\nfeature_name = X.columns[feature_index]\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of actual vs. predicted\nplt.scatter(X_test[feature_name], y_test, color='blue', label='Actual Values')\nplt.scatter(X_test[feature_name], y_pred, color='red', marker='x', label='Predicted Values')\n\n\nplt.title('Regression Fit for Feature: ' + feature_name)\nplt.xlabel(feature_name)\nplt.ylabel('Selling Price')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nSecond regression model with transformation\nnow, lets transform the selling price and km_driven columns using log() function and create a new regression model\n\ncars['selling_price'] = np.log1p(cars['selling_price'])\ncars['km_driven'] = np.log1p(cars['km_driven'])\n\n\n# Define predictors (X) and target variable (y)\nX = cars.drop('selling_price', axis=1)\ny = cars['selling_price']\nX_train = X_train.dropna()\ny_train = y_train[X_train.index]  # Keep corresponding y values\n\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Drop rows with NaN values from X_train\nX_train = X_train.dropna()\n# Synchronize y_train with the updated X_train\ny_train = y_train.loc[X_train.index]\nX_test = X_test.dropna()\ny_test = y_test.loc[X_test.index]\n\n\n# Create and train the Linear Regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = lin_reg.predict(X_test)\n\n# Evaluate the model's performance\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 (coefficient of determination): {r2:.2f}\")\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse:.2f}\")\n\n# Model's coefficients and intercept\nprint(f\"Intercept: {lin_reg.intercept_}\")\nprint(f\"Coefficients: {lin_reg.coef_}\")\n\nR^2 (coefficient of determination): 0.60\nMean Squared Error: 0.27\nIntercept: 14.408622302677129\nCoefficients: [-4.35206639e-01  7.52823926e-02  1.32585312e-03  1.64254578e-04\n -9.24979659e-02 -1.18337658e-02 -6.49937878e-02  4.32081971e-02\n -1.68184483e-02 -7.34963940e-02 -1.12130018e-02 -7.46682688e-01\n -4.84720247e-02]\n\n\n\n# Selecting the first feature for demonstration\nfeature_index = 0\nfeature_name = X.columns[feature_index]\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of actual vs. predicted\nplt.scatter(X_test[feature_name], y_test, color='blue', label='Actual Values')\nplt.scatter(X_test[feature_name], y_pred, color='red', marker='x', label='Predicted Values')\n\n\nplt.title('Regression Fit for Feature: ' + feature_name)\nplt.xlabel(feature_name)\nplt.ylabel('Selling Price')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\nby doing the transformation, we gained around 23% in the fit.\nHence the final equation is:\n\\begin{equation}\n\\begin{split}\n\\text{selling\\_price} = & 14.4086 \\\\\n& - 0.4352 \\times \\text{km\\_driven} \\\\\n& + 0.0753 \\times \\text{mileage} \\\\\n& + 0.0013 \\times \\text{engine} \\\\\n& + 0.0002 \\times \\text{fuel\\_Diesel} \\\\\n& - 0.0925 \\times \\text{fuel\\_LPG} \\\\\n& - 0.0118 \\times \\text{fuel\\_Petrol} \\\\\n& - 0.0650 \\times \\text{seller\\_type\\_Individual} \\\\\n& + 0.0432 \\times \\text{seller\\_type\\_Trustmark Dealer} \\\\\n& - 0.0168 \\times \\text{transmission\\_Manual} \\\\\n& - 0.0735 \\times \\text{owner\\_Fourth \\& Above Owner} \\\\\n& - 0.0112 \\times \\text{owner\\_Second Owner} \\\\\n& - 0.7467 \\times \\text{owner\\_Test Drive Car} \\\\\n& - 0.0485 \\times \\text{owner\\_Third Owner}\n\\end{split}\n\\end{equation}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine_learning_basics",
    "section": "",
    "text": "Introduction to Probability Theory\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMultiple Linear Regression: Application on FARS Dataset\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I am Ahmed Aredah, a PhD student specializing in Civil Engineering with a focus on transportation. Concurrently, I’m also pursuing an MSc in Computer Science. My academic journey has equipped me with a unique blend of knowledge, bridging the gap between the worlds of transportation and computer science.\nI take pride in being the developer behind NeTrainSim, an open-source network train simulator. NeTrainSim is a collaborative effort between Virginia Tech (VT), North Carolina State University (NC), and Deutsch Bahn (DB). The project was supervised and steered by the esteemed Prof. Hesham Rakha. This initiative is a testament to my passion for contributing to the community and my commitment to advancing the field of transportation through technological innovations and collaborative research.\nWithin this blog, I will be showing the basics of machine learning techniques without going into too much of techniqal complexities. In addition, I will shed the lights on a quick overview of each technique with a little bit of some code.\n\n\\text{selling\\_price} = 14.4086 - 0.4352 \\times \\text{km\\_driven} + 0.0753 \\times \\text{mileage} + 0.0013 \\times \\text{engine} + 0.0002 \\times \\text{fuel\\_Diesel} - 0.0925 \\times \\text{fuel\\_LPG} - 0.0118 \\times \\text{fuel\\_Petrol} - 0.0650 \\times \\text{seller\\_type\\_Individual} + 0.0432 \\times \\text{seller\\_type\\_Trustmark Dealer} - 0.0168 \\times \\text{transmission\\_Manual} - 0.0735 \\times \\text{owner\\_Fourth & Above Owner} - 0.0112 \\times \\text{owner\\_Second Owner} - 0.7467 \\times \\text{owner\\_Test Drive Car} - 0.0485 \\times \\text{owner\\_Third Owner}"
  },
  {
    "objectID": "posts/1.Probabilities/Probability Theory.html",
    "href": "posts/1.Probabilities/Probability Theory.html",
    "title": "Introduction to Probability Theory",
    "section": "",
    "text": "Probability theory and random variables are fundamental concepts in machine learning, providing the mathematical framework for dealing with uncertainty and variability in data. In this blog post, we will explore these concepts and see how they are applied in the field of machine learning."
  },
  {
    "objectID": "posts/1.Probabilities/Probability Theory.html#introduction",
    "href": "posts/1.Probabilities/Probability Theory.html#introduction",
    "title": "Introduction to Probability Theory",
    "section": "",
    "text": "Probability theory and random variables are fundamental concepts in machine learning, providing the mathematical framework for dealing with uncertainty and variability in data. In this blog post, we will explore these concepts and see how they are applied in the field of machine learning."
  },
  {
    "objectID": "posts/1.Probabilities/Probability Theory.html#probability-theory",
    "href": "posts/1.Probabilities/Probability Theory.html#probability-theory",
    "title": "Introduction to Probability Theory",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability theory is a branch of mathematics that deals with the likelihood or chance of different outcomes. It is used in machine learning to model and make predictions about uncertain events. Some of the key concepts in probability theory include:\n\nProbability Distribution: A probability distribution describes how the values of a random variable are distributed. It can be discrete or continuous, depending on the type of variable.\nExpected Value: The expected value is the mean or average value of a random variable, calculated by taking the weighted average of all possible values.\nVariance and Standard Deviation: These measures describe the spread or dispersion of a random variable’s values."
  },
  {
    "objectID": "posts/1.Probabilities/Probability Theory.html#random-variables",
    "href": "posts/1.Probabilities/Probability Theory.html#random-variables",
    "title": "Introduction to Probability Theory",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a variable that takes on different values based on the outcome of a random process. In machine learning, random variables are used to represent the features and target variable in a dataset. They can be classified into two types:\n\nDiscrete Random Variables: These variables take on a finite number of values. For example, the outcome of a coin toss can be represented as a discrete random variable with two possible values: heads or tails.\nContinuous Random Variables: These variables can take on an infinite number of values within a given range. For example, the height of a person can be represented as a continuous random variable."
  },
  {
    "objectID": "posts/1.Probabilities/Probability Theory.html#application-in-machine-learning",
    "href": "posts/1.Probabilities/Probability Theory.html#application-in-machine-learning",
    "title": "Introduction to Probability Theory",
    "section": "Application in Machine Learning",
    "text": "Application in Machine Learning\nIn machine learning, probability theory and random variables are used to model and understand the uncertainty in data. Here are some ways they are applied:\n\nRandom Forests: Random forests are a type of ensemble learning method that uses a collection of decision trees to make predictions. The “random” in random forests refers to the random selection of features and data points used to build each tree."
  },
  {
    "objectID": "posts/1.Probabilities/Probability Theory.html#code-examples",
    "href": "posts/1.Probabilities/Probability Theory.html#code-examples",
    "title": "Introduction to Probability Theory",
    "section": "Code Examples",
    "text": "Code Examples\nIn the following Python examples, we will illustrate the key concepts of probability theory and random variables discussed above. Here’s what we will be doing:\n\nProbability Distribution: We’ll create a normal distribution, which is a type of probability distribution, and visualize it using a graph. This will demonstrate how data can be distributed around a mean value.\n\nExpected Value and Variance: From the normal distribution, we’ll calculate the expected value (mean) and variance, which give us a sense of the central tendency and spread of the data.\n\nDiscrete Random Variable: We’ll simulate a coin toss, which is an example of a discrete random variable, to show how we can represent events with two or more distinct outcomes.\nContinuous Random Variable: Finally, we’ll simulate the heights of people, which is an example of a continuous random variable, to demonstrate how we can represent events with an infinite range of possible outcomes.\n\nThrough these examples, we’ll gain a practical understanding of these concepts and see how they can be applied to real-world data.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\n# Probability Distribution: Example with a Normal Distribution\nmu, sigma = 0, 0.1  # mean and standard deviation\ns = np.random.normal(mu, sigma, 1000)\n\n# Plot the histogram of the sample\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1 / (sigma * np.sqrt(2 * np.pi)) *\n         np.exp(-(bins - mu) ** 2 / (2 * sigma ** 2)),\n         linewidth=2, color='r')\nplt.title('Probability Distribution: Normal Distribution')\nplt.show()\n\n# Expected Value and Variance\nexpected_value = np.mean(s)\nvariance = np.var(s)\nprint(f'Expected Value: {expected_value}')\nprint(f'Variance: {variance}')\n\n\n\n\nExpected Value: -0.0011666313731559523\nVariance: 0.009747766798048644\n\n\nIn this Python code, we generated random numbers to demonstrate what we learned earlier. First, we showed what a normal distribution looks like by drawing a bell-shaped curve, and then we calculated the average (mean) and spread (variance) of the numbers.\n\n# Discrete Random Variable: Example with a Coin Toss\ncoin_toss = np.random.choice(['Heads', 'Tails'], 1000)\nunique, counts = np.unique(coin_toss, return_counts=True)\ncoin_toss_prob = dict(zip(unique, counts / len(coin_toss)))\nprint(f'Coin Toss Probabilities: {coin_toss_prob}')\n\nCoin Toss Probabilities: {'Heads': 0.479, 'Tails': 0.521}\n\n\nHere, we simulated flipping a coin a thousand times and showed the likelihood of getting heads or tails. Which turns out to be ~0.5 for both cases.\n\n# Continuous Random Variable: Example with Heights of People\nheights = np.random.normal(170, 10, 1000)  # mean height = 170 cm, std deviation = 10 cm\nmean_height = np.mean(heights)\nvar_height = np.var(heights)\nprint(f'Mean Height: {mean_height} cm')\nprint(f'Variance of Heights: {var_height} cm^2')\n\nMean Height: 169.9326452743008 cm\nVariance of Heights: 104.72824250963349 cm^2\n\n\nHere, we simulated the heights of a thousand people, calculated the average height, and how much the heights vary from each other. This is just like how we use these concepts in machine learning to understand and predict different events or outcomes based on data.\nIn conclusion, machine learning is like teaching a computer to make educated guesses. It looks at patterns and learns how likely different outcomes are, much like how we use probability theory to predict future events. By using random variables, which are values that can change randomly, machine learning models can consider a range of possibilities and make predictions that can be generalized to new, unseen data. This combination of probability theory and random variables is what allows machine learning models to effectively learn from data and make accurate predictions."
  }
]