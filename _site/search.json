[
  {
    "objectID": "posts/3.Classification/2-KNN.html",
    "href": "posts/3.Classification/2-KNN.html",
    "title": "3.2-k-Nearest Neighbour (k-NN)",
    "section": "",
    "text": "The k-Nearest Neighbour (k-NN) algorithm is a simple, intuitive method used for classification and regression tasks. Unlike model-based algorithms, it doesn’t explicitly learn a model. Instead, it memorizes the training dataset and makes predictions by finding the ‘k’ training examples closest to a given input."
  },
  {
    "objectID": "posts/3.Classification/2-KNN.html#overview",
    "href": "posts/3.Classification/2-KNN.html#overview",
    "title": "3.2-k-Nearest Neighbour (k-NN)",
    "section": "",
    "text": "The k-Nearest Neighbour (k-NN) algorithm is a simple, intuitive method used for classification and regression tasks. Unlike model-based algorithms, it doesn’t explicitly learn a model. Instead, it memorizes the training dataset and makes predictions by finding the ‘k’ training examples closest to a given input."
  },
  {
    "objectID": "posts/3.Classification/2-KNN.html#mathematical-description",
    "href": "posts/3.Classification/2-KNN.html#mathematical-description",
    "title": "3.2-k-Nearest Neighbour (k-NN)",
    "section": "Mathematical Description",
    "text": "Mathematical Description\nGiven a new observation \\mathbf{x}_0, the k-NN algorithm identifies ‘k’ points from the training dataset that are closest to \\mathbf{x}_0. For classification, the mode (most frequent class) among these k points is taken as the predicted class for \\mathbf{x}_0. For regression, the average (or median) output value of these k points is used as the prediction.\nThe distance between two points can be computed using various metrics, with the Euclidean distance being the most common:\n\\begin{equation}\nd(\\mathbf{x}_i, \\mathbf{x}_j) = \\sqrt{\\sum_{l=1}^{n} (x_{il} - x_{jl})^2}\n\\end{equation}\nwhere:\n\n\\mathbf{x}_i and \\mathbf{x}_j are two points in n-dimensional space.\n$d(_i, \\mathbf{x}_j)$ is the Euclidean distance between \\mathbf{x}_i and \\mathbf{x}_j.\n\n\nWeighted k-NN\nA variation of k-NN is the weighted k-NN, where weights are assigned to the k neighbours based on their distance to the query point. Closer neighbors will have a larger influence on the prediction than farther ones. The weights are typically computed using the inverse of the distance:\n\\begin{equation}\nw_i = \\frac{1}{d(\\mathbf{x}_0, \\mathbf{x}_i)^2}\n\\end{equation}"
  },
  {
    "objectID": "posts/3.Classification/2-KNN.html#application-on-the-breast-cancer-wisconsin-dataset",
    "href": "posts/3.Classification/2-KNN.html#application-on-the-breast-cancer-wisconsin-dataset",
    "title": "3.2-k-Nearest Neighbour (k-NN)",
    "section": "Application on the Breast Cancer Wisconsin Dataset",
    "text": "Application on the Breast Cancer Wisconsin Dataset\nWe will be using the same dataset we used before in the LogisticRegression. By comparing the results from both logistic regression and k-NN, we can evaluate the strengths and weaknesses of each approach and determine which is more suitable for this particular dataset.\n\n#import required libraries\nimport pandas as pd\nimport os\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\n\n\n#inline plots\n%matplotlib inline\n\n# Set Seaborn style to \"whitegrid\" for a white background with grid lines\nsns.set_style(\"whitegrid\")\n\n#supress warnings!\nwarnings.simplefilter(action='ignore', category=Warning)\n# Set the display option to show all columns\npd.set_option('display.max_columns', None)\n\nprint(\"required libraries loaded successfully!\")\n\nrequired libraries loaded successfully!\n\n\n\n# Load the dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nfeature_names = data.feature_names\n\n# Convert the data to a pandas DataFrame\ndf = pd.DataFrame(X, columns=feature_names)\ndf['target'] = y\n\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the classifier\nclf = KNeighborsClassifier(n_neighbors=5)\n\n# Perform 5-fold cross-validation\ncv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n\n# Print the cross-validation scores\nprint(\"Cross-validation scores:\", cv_scores)\n\n# Print the average cross-validation score\nprint(\"Average cross-validation score:\", cv_scores.mean())\n\n# Train the classifier on the training data\nclf.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = clf.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nCross-validation scores: [0.93406593 0.9010989  0.95604396 0.9010989  0.92307692]\nAverage cross-validation score: 0.9230769230769231\nAccuracy: 0.956140350877193\n\n\n\n# Plot the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\n\n# Reduce the dimensionality of the dataset to 2 dimensions using PCA so we can visualize it\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n\n# Initialize the classifier\nclf = KNeighborsClassifier(n_neighbors=5)\n\n# Train the classifier on the training data\nclf.fit(X_train, y_train)\n\n# Create a meshgrid for the plot\nx_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\ny_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 1),\n                     np.arange(y_min, y_max, 1))\n\n# Get the predictions for each point in the meshgrid\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the prediction region\nplt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n\n# Plot the data points\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y+6, cmap='viridis')\n\n# Add a legend\nlegend = plt.legend(*scatter.legend_elements(), title='Classes')\nplt.gca().add_artist(legend)\n\n# Show the plot\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Prediction Region for Breast Cancer Dataset')\nplt.show()"
  },
  {
    "objectID": "posts/2.Regression/2-SupportVectorRegression.html",
    "href": "posts/2.Regression/2-SupportVectorRegression.html",
    "title": "2.2-Support Vector Regression",
    "section": "",
    "text": "Support Vector Regression (SVR) is used to model the relationship between a dependent variable (y) and multiple independent variables (x_1, x_2, , x_n). The objective of SVR is to find a function (f(x)) that has at most () deviation from the actually obtained targets (y_i) for all the input samples (x_i), and at the same time is as flat as possible.\nThe SVR function can be mathematically represented as follows: \\begin{equation}\nf(x) = \\langle w, x \\rangle + b\n\\end{equation}\nwhere:\n\n(w) is the weight vector.\n(x) is the input vector.\n(b) is the bias.\n(w, x ) represents the dot product of (w) and (x).\n\nThe goal of SVR is to find the optimal values of (w) and (b) that minimize the following cost function: \\begin{equation}\n\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} \\left(\\max(0, |y_i - \\langle w, x_i \\rangle - b| - \\epsilon)\\right)\n\\end{equation}\nwhere:\n\n(C) is the regularization parameter, which controls the trade-off between achieving a low training error and a low testing error.\n() is the insensitivity tube within which no penalty is associated in the training loss function with points predicted within a distance () from the actual value.\n\n\n\n\nimage.png\n\n\n\n\nIn cases where the relationship between the dependent and independent variables is non-linear, SVR can be extended to handle non-linear relationships using the kernel trick. The kernel trick involves transforming the input space into a higher-dimensional space where a linear relationship between the dependent and independent variables can be found. Mathematically, this can be represented as follows: \\begin{equation}\nf(x) = \\langle w, \\phi(x) \\rangle + b\n\\end{equation}\nwhere:\n\n((x)) is the transformed input vector.\n\nThe cost function becomes: \\begin{equation}\n\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} \\left(\\max(0, |y_i - \\langle w, \\phi(x_i) \\rangle - b| - \\epsilon)\\right)\n\\end{equation}\nA common kernel used in SVR is the Gaussian Radial Basis Function (RBF) kernel, which is defined as follows: \\begin{equation}\nK(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\n\\end{equation}\nwhere:\n\n(K(x_i, x_j)) is the kernel function that computes the dot product ((x_i), (x_j) ).\n() is the bandwidth parameter of the RBF kernel."
  },
  {
    "objectID": "posts/2.Regression/2-SupportVectorRegression.html#mathematical-formulation",
    "href": "posts/2.Regression/2-SupportVectorRegression.html#mathematical-formulation",
    "title": "2.2-Support Vector Regression",
    "section": "",
    "text": "Support Vector Regression (SVR) is used to model the relationship between a dependent variable (y) and multiple independent variables (x_1, x_2, , x_n). The objective of SVR is to find a function (f(x)) that has at most () deviation from the actually obtained targets (y_i) for all the input samples (x_i), and at the same time is as flat as possible.\nThe SVR function can be mathematically represented as follows: \\begin{equation}\nf(x) = \\langle w, x \\rangle + b\n\\end{equation}\nwhere:\n\n(w) is the weight vector.\n(x) is the input vector.\n(b) is the bias.\n(w, x ) represents the dot product of (w) and (x).\n\nThe goal of SVR is to find the optimal values of (w) and (b) that minimize the following cost function: \\begin{equation}\n\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} \\left(\\max(0, |y_i - \\langle w, x_i \\rangle - b| - \\epsilon)\\right)\n\\end{equation}\nwhere:\n\n(C) is the regularization parameter, which controls the trade-off between achieving a low training error and a low testing error.\n() is the insensitivity tube within which no penalty is associated in the training loss function with points predicted within a distance () from the actual value.\n\n\n\n\nimage.png\n\n\n\n\nIn cases where the relationship between the dependent and independent variables is non-linear, SVR can be extended to handle non-linear relationships using the kernel trick. The kernel trick involves transforming the input space into a higher-dimensional space where a linear relationship between the dependent and independent variables can be found. Mathematically, this can be represented as follows: \\begin{equation}\nf(x) = \\langle w, \\phi(x) \\rangle + b\n\\end{equation}\nwhere:\n\n((x)) is the transformed input vector.\n\nThe cost function becomes: \\begin{equation}\n\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} \\left(\\max(0, |y_i - \\langle w, \\phi(x_i) \\rangle - b| - \\epsilon)\\right)\n\\end{equation}\nA common kernel used in SVR is the Gaussian Radial Basis Function (RBF) kernel, which is defined as follows: \\begin{equation}\nK(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\n\\end{equation}\nwhere:\n\n(K(x_i, x_j)) is the kernel function that computes the dot product ((x_i), (x_j) ).\n() is the bandwidth parameter of the RBF kernel."
  },
  {
    "objectID": "posts/2.Regression/2-SupportVectorRegression.html#application-on-cars-selling-dataset",
    "href": "posts/2.Regression/2-SupportVectorRegression.html#application-on-cars-selling-dataset",
    "title": "2.2-Support Vector Regression",
    "section": "Application on Cars selling Dataset",
    "text": "Application on Cars selling Dataset\nFor this blog post, we will be applying SVR to predict car selling prices based on various features such as milage, power, etc. The dataset used here will be the same as the one used for the Linear Regression model to compare the resultant model.\n\n#import required libraries\nimport pandas as pd\nimport os\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#inline plots\n%matplotlib inline\n\n# Set Seaborn style to \"whitegrid\" for a white background with grid lines\nsns.set_style(\"whitegrid\")\n\n#supress warnings!\nwarnings.simplefilter(action='ignore', category=Warning)\n# Set the display option to show all columns\npd.set_option('display.max_columns', None)\n\nprint(\"required libraries loaded successfully!\")\n\nrequired libraries loaded successfully!\n\n\nLet’s quickly load the dataset and clean it as we did in the linear regression post\n\n# Load the cars dataset\ncars = pd.read_csv(\"https://raw.githubusercontent.com/AhmedAredah/MachineLearningBasics/main/data/cars.csv\")\n\n# Drop rows where any cell contains NA or NAN \ncars = cars.dropna()\n\n# List of columns to process\ncolumns_to_process = ['mileage', 'engine', 'max_power']\n\nfor column in columns_to_process:\n    # Convert the column to string type\n    cars[column] = cars[column].astype(str)\n    \n    # Extract the first numerical value (assumes format is \"value unit\")\n    cars[column] = cars[column].str.split().str[0]\n    \n    # Convert those values to float, set others to NaN if they can't be converted\n    cars[column] = pd.to_numeric(cars[column], errors='coerce')\n\nimport re\n\n# Function to extract the numeric part before 'Nm'\ndef extract_torque_value(s):\n    match = re.search(r'(\\d+)Nm', s)\n    return float(match.group(1)) if match else None\n\n# Apply the function to the torque column\ncars['torque'] = cars['torque'].apply(extract_torque_value)\ncars.dropna()\ncars = cars.drop([\"year\", \"seats\", \"max_power\"], axis=1)\n\n# Preprocessing: One-hot encode categorical variables\ncategorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']\none_hot = OneHotEncoder(drop='first', sparse=False)  # drop='first' to avoid multicollinearity\nencoded_features = one_hot.fit_transform(cars[categorical_columns])\nencoded_df = pd.DataFrame(encoded_features, columns=one_hot.get_feature_names_out(categorical_columns))\n\n# Concatenate encoded features with the original dataframe\ncars = pd.concat([cars, encoded_df], axis=1)\n\n# Drop the original categorical columns and other non-numeric columns\ncars = cars.drop(columns=categorical_columns + ['name', 'torque'])\n\n# Define predictors (X) and target variable (y)\nX = cars.drop('selling_price', axis=1)\ny = cars['selling_price']\nX = X.fillna(X.mean())\ny = y.fillna(y.mean())\n\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the Support Vector Regression model\nsvr_reg = SVR(kernel='linear')\nsvr_reg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = svr_reg.predict(X_test)\n\n# Evaluate the model's performance\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 (coefficient of determination): {r2:.2f}\")\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse:.2f}\")\n\nR^2 (coefficient of determination): 0.27\nMean Squared Error: 479844290934.03\n\n\n\n# Selecting the first feature for demonstration\nfeature_index = 0\nfeature_name = X.columns[feature_index]\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of actual vs. predicted\nplt.scatter(X_test[feature_name], y_test, color='skyblue', label='Actual Values')\nplt.scatter(X_test[feature_name], y_pred, color='red', marker='x', label='Predicted Values')\n\n\nplt.title('Regression Fit for Feature: ' + feature_name)\nplt.xlabel(feature_name)\nplt.ylabel('Selling Price')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n# Calculate the errors\nerrors = y_test - y_pred\n\n# Plot the errors\nplt.figure(figsize=(10, 6))\nplt.plot(errors, marker='o', linestyle='')\nplt.axhline(0, color='r', linestyle='--')\nplt.title('Errors of the SVR Model')\nplt.xlabel('Observation')\nplt.ylabel('Error')\nplt.grid(True)\nplt.show()\n\n\n\n\nLooking at the “Errors of the SVR Model” plot, the following observations can be made:\n\nHeteroscedasticity: The spread of the residuals is not constant across the range of observations. There’s a notable concentration of residuals around the zero-error line, but the spread increases for certain sections of the observation range. This suggests the presence of heteroscedasticity, which means the model’s performance is inconsistent across different segments of the data.\nOutliers: There are several points that lie far from the zero-error line, indicating potential outliers in the dataset. These outliers can have a significant impact on the model’s overall performance and might be the cause of some of the model’s inaccuracies.\nTrend in Residuals: While there’s a central concentration of residuals around the zero line, some scattered patterns can be observed, suggesting potential non-linearities or other trends in the data that the current SVR model might not be capturing.\nBias: The majority of the residuals seem to be concentrated slightly above and below the zero-error line, but the distribution appears fairly symmetric. This suggests that there’s no significant systematic bias in the model’s predictions.\nScale of Errors: Some errors are notably large, reaching values in the millions. This indicates that for certain observations, the SVR model’s predictions can be significantly off the mark.\n\nIn conclusion, while the SVR model seems to perform adequately for a majority of the observations (as indicated by the residuals clustered around the zero line), there’s room for improvement, especially considering the presence of outliers, the increasing spread of residuals in certain observation ranges, and potential trends in the errors. Adjustments to the model, such as addressing outliers, feature engineering, and hyperparameter tuning, could lead to better and more consistent performance.\n\n\nSecond regression model with transformation\nnow, lets transform the selling price and km_driven columns using log() function and create a new regression model\n\ncars['selling_price'] = np.log1p(cars['selling_price'])\ncars['km_driven'] = np.log1p(cars['km_driven'])\n\n# Define predictors (X) and target variable (y)\nX = cars.drop('selling_price', axis=1)\ny = cars['selling_price']\nX_train = X_train.dropna()\ny_train = y_train[X_train.index]  # Keep corresponding y values\n\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Drop rows with NaN values from X_train\nX_train = X_train.dropna()\n# Synchronize y_train with the updated X_train\ny_train = y_train.loc[X_train.index]\nX_test = X_test.dropna()\ny_test = y_test.loc[X_test.index]\n\n# Create and train the Support Vector Regression model\nsvr_reg = SVR(kernel='linear',) \nsvr_reg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = svr_reg.predict(X_test)\n\n# Evaluate the model's performance\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 (coefficient of determination): {r2:.2f}\")\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse:.2f}\")\n\nR^2 (coefficient of determination): 0.02\nMean Squared Error: 0.66\n\n\n\n# Selecting the first feature for demonstration\nfeature_index = 0\nfeature_name = X.columns[feature_index]\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of actual vs. predicted\nplt.scatter(X_test[feature_name], y_test, color='skyblue', label='Actual Values')\nplt.scatter(X_test[feature_name], y_pred, color='red', marker='x', label='Predicted Values')\n\n\nplt.title('Regression Fit for Feature: ' + feature_name)\nplt.xlabel(feature_name)\nplt.ylabel('Selling Price')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n# Calculate residuals\nresiduals = y_test - y_pred\n\n# Plot residuals\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.title('Residuals vs. Predicted Values')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\nObserving the “Residuals vs. Predicted Values” plot, we can make the following comments:\n\nHomoscedasticity: The residuals seem to be spread relatively evenly around the zero line for most of the predicted values. This suggests that the model’s errors are generally consistent across the range of predicted values, indicating homoscedasticity. This is a desirable property as it means the model’s performance is consistent across different predicted values.\nNo Clear Bias: The residuals are dispersed both above and below the zero line in a relatively symmetric fashion. This suggests that the model does not exhibit a clear systematic bias in either overestimating or underestimating the target values.\nPossible Non-Linearity: The somewhat “U” shaped pattern of the residuals suggests that there might be some non-linear relationships in the data that the model is not fully capturing. This indicates that a model with a non-linear transformation or a different type of model might perform better.\nOutliers: There are some residuals that deviate significantly from the zero line, especially on the lower and upper end of the predicted values. These could be outliers or instances where the model’s predictions are considerably inaccurate.\nDensity of Residuals: A dense clustering of residuals around the zero line, especially in the mid-range of predicted values, suggests that the model is making accurate predictions for a large portion of the data.\n\nIn summary, the model appears to be doing a reasonably good job for many of the predicted values, as indicated by the dense cluster of residuals around the zero line. However, the curvature observed in the residuals’ pattern hints at potential non-linearities in the data that might be addressed with further feature engineering, model adjustments, or considering a different modeling approach. The presence of significant outliers also indicates areas where the model’s predictions could be improved."
  },
  {
    "objectID": "posts/2.Regression/2-SupportVectorRegression.html#conclusion",
    "href": "posts/2.Regression/2-SupportVectorRegression.html#conclusion",
    "title": "2.2-Support Vector Regression",
    "section": "Conclusion",
    "text": "Conclusion\n1. Performance Metrics:\n\nR^2 (Coefficient of Determination): The Linear Regression model has an R^2 value of 0.60, suggesting that it explains 60% of the variance in the target variable, which is a relatively decent fit. On the other hand, the SVR model has a low R^2 value of 0.02, indicating it only explains 2% of the variance, which is quite poor.\nMean Squared Error (MSE): The Linear Regression model’s MSE of 0.27 is considerably lower than the SVR’s 0.66. A lower MSE suggests that the Linear Regression model’s predictions are closer to the actual values, on average, compared to the SVR model.\n\n2. Residual Plots:\nSupport Vector Regression Model:\nThe residuals display a noticeable “U” shaped pattern, suggesting potential non-linear relationships in the data that the SVR model may not be capturing. The residuals have a wider spread, indicating higher variability in the errors.\nLinear Regression Model:\nThe residuals are more centered around the zero line and are more clustered, suggesting the predictions are generally more accurate and consistent. There’s less of a discernible pattern in the residuals, although a slight curvature suggests there could be minor non-linearities."
  },
  {
    "objectID": "posts/1.Probabilities/2-Probability Theory.html",
    "href": "posts/1.Probabilities/2-Probability Theory.html",
    "title": "1.2-Probability Theory",
    "section": "",
    "text": "Probability theory and random variables are fundamental concepts in machine learning, providing the mathematical framework for dealing with uncertainty and variability in data. In this blog post, we will explore these concepts and see how they are applied in the field of machine learning."
  },
  {
    "objectID": "posts/1.Probabilities/2-Probability Theory.html#probability-theory",
    "href": "posts/1.Probabilities/2-Probability Theory.html#probability-theory",
    "title": "1.2-Probability Theory",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability theory is a branch of mathematics that deals with the likelihood or chance of different outcomes. It is used in machine learning to model and make predictions about uncertain events. Some of the key concepts in probability theory include:\n\nProbability Distribution: A probability distribution describes how the values of a random variable are distributed. It can be discrete or continuous, depending on the type of variable.\nExpected Value: The expected value is the mean or average value of a random variable, calculated by taking the weighted average of all possible values.\nVariance and Standard Deviation: These measures describe the spread or dispersion of a random variable’s values."
  },
  {
    "objectID": "posts/1.Probabilities/2-Probability Theory.html#random-variables",
    "href": "posts/1.Probabilities/2-Probability Theory.html#random-variables",
    "title": "1.2-Probability Theory",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a variable that takes on different values based on the outcome of a random process. In machine learning, random variables are used to represent the features and target variable in a dataset. They can be classified into two types:\n\nDiscrete Random Variables: These variables take on a finite number of values. For example, the outcome of a coin toss can be represented as a discrete random variable with two possible values: heads or tails.\nContinuous Random Variables: These variables can take on an infinite number of values within a given range. For example, the height of a person can be represented as a continuous random variable."
  },
  {
    "objectID": "posts/1.Probabilities/2-Probability Theory.html#application-in-machine-learning",
    "href": "posts/1.Probabilities/2-Probability Theory.html#application-in-machine-learning",
    "title": "1.2-Probability Theory",
    "section": "Application in Machine Learning",
    "text": "Application in Machine Learning\nIn machine learning, probability theory and random variables are used to model and understand the uncertainty in data. Here are some ways they are applied:\n\nRandom Forests: Random forests are a type of ensemble learning method that uses a collection of decision trees to make predictions. The “random” in random forests refers to the random selection of features and data points used to build each tree."
  },
  {
    "objectID": "posts/1.Probabilities/2-Probability Theory.html#code-examples",
    "href": "posts/1.Probabilities/2-Probability Theory.html#code-examples",
    "title": "1.2-Probability Theory",
    "section": "Code Examples",
    "text": "Code Examples\nIn the following Python examples, we will illustrate the key concepts of probability theory and random variables discussed above. Here’s what we will be doing:\n\nProbability Distribution: We’ll create a normal distribution, which is a type of probability distribution, and visualize it using a graph. This will demonstrate how data can be distributed around a mean value.\n\nExpected Value and Variance: From the normal distribution, we’ll calculate the expected value (mean) and variance, which give us a sense of the central tendency and spread of the data.\n\nDiscrete Random Variable: We’ll simulate a coin toss, which is an example of a discrete random variable, to show how we can represent events with two or more distinct outcomes.\nContinuous Random Variable: Finally, we’ll simulate the heights of people, which is an example of a continuous random variable, to demonstrate how we can represent events with an infinite range of possible outcomes.\n\nThrough these examples, we’ll gain a practical understanding of these concepts and see how they can be applied to real-world data.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\n# Probability Distribution: Example with a Normal Distribution\nmu, sigma = 0, 0.1  # mean and standard deviation\ns = np.random.normal(mu, sigma, 1000)\n\n# Plot the histogram of the sample\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1 / (sigma * np.sqrt(2 * np.pi)) *\n         np.exp(-(bins - mu) ** 2 / (2 * sigma ** 2)),\n         linewidth=2, color='r')\nplt.title('Probability Distribution: Normal Distribution')\nplt.show()\n\n# Expected Value and Variance\nexpected_value = np.mean(s)\nvariance = np.var(s)\nprint(f'Expected Value: {expected_value}')\nprint(f'Variance: {variance}')\n\n\n\n\nExpected Value: -0.0011666313731559523\nVariance: 0.009747766798048644\n\n\nIn this Python code, we generated random numbers to demonstrate what we learned earlier. First, we showed what a normal distribution looks like by drawing a bell-shaped curve, and then we calculated the average (mean) and spread (variance) of the numbers.\n\n# Discrete Random Variable: Example with a Coin Toss\ncoin_toss = np.random.choice(['Heads', 'Tails'], 1000)\nunique, counts = np.unique(coin_toss, return_counts=True)\ncoin_toss_prob = dict(zip(unique, counts / len(coin_toss)))\nprint(f'Coin Toss Probabilities: {coin_toss_prob}')\n\nCoin Toss Probabilities: {'Heads': 0.479, 'Tails': 0.521}\n\n\nHere, we simulated flipping a coin a thousand times and showed the likelihood of getting heads or tails. Which turns out to be ~0.5 for both cases.\n\n# Continuous Random Variable: Example with Heights of People\nheights = np.random.normal(170, 10, 1000)  # mean height = 170 cm, std deviation = 10 cm\nmean_height = np.mean(heights)\nvar_height = np.var(heights)\nprint(f'Mean Height: {mean_height} cm')\nprint(f'Variance of Heights: {var_height} cm^2')\n\nMean Height: 169.9326452743008 cm\nVariance of Heights: 104.72824250963349 cm^2\n\n\nHere, we simulated the heights of a thousand people, calculated the average height, and how much the heights vary from each other. This is just like how we use these concepts in machine learning to understand and predict different events or outcomes based on data.\nIn conclusion, machine learning is like teaching a computer to make educated guesses. It looks at patterns and learns how likely different outcomes are, much like how we use probability theory to predict future events. By using random variables, which are values that can change randomly, machine learning models can consider a range of possibilities and make predictions that can be generalized to new, unseen data. This combination of probability theory and random variables is what allows machine learning models to effectively learn from data and make accurate predictions."
  },
  {
    "objectID": "posts/1.Probabilities/0-Introduction.html",
    "href": "posts/1.Probabilities/0-Introduction.html",
    "title": "1.0-Introduction and Basics",
    "section": "",
    "text": "Machine learning is where the structured world of statistics meets the ever-growing and complex universe of data. In this digital age, machine learning takes statistical tools and uses them to teach computers to find patterns and make decisions based on data. This is a key part of data science, which aims to sift through large sets of information to find useful insights. While traditional statistics often focuses on testing theories and making predictions from samples, machine learning uses these ideas to create models that can predict future events based on past data. Data analytics is another piece of the puzzle, using these models to look closely at data and find valuable information that businesses can use to make better decisions.\nThe world of statistics that feeds into machine learning is divided into two main approaches: Frequentist and Bayesian. Frequentist statistics predict future events by looking at past frequencies without making assumptions. This approach is popular in machine learning for its straightforward application to large datasets and its ability to provide clear answers without prior knowledge. Meanwhile, Bayesian statistics use past information as a starting point and update this as new data comes in, making it very useful for situations where data is limited or uncertain. Each approach has its place in machine learning: Frequentist methods are strong and work well on a large scale, while Bayesian methods are flexible and incorporate prior knowledge into their predictions. Together, these statistical methods power the engines of data science and analytics, helping us to understand and navigate the complex data landscapes of today’s world.\nBefore we delve into the realm of machine learning, it’s essential to build a foundational understanding of the data we work with. Data, the raw material of machine learning, comes in various shapes and types, each with its own nuances and implications for how we’ll eventually teach machines to interpret and learn from it."
  },
  {
    "objectID": "posts/1.Probabilities/0-Introduction.html#data-set-types",
    "href": "posts/1.Probabilities/0-Introduction.html#data-set-types",
    "title": "1.0-Introduction and Basics",
    "section": "Data Set Types",
    "text": "Data Set Types\nAny data set could be classified by either of the following:\n\nUnivariate data sets are the simplest form, containing just one variable. Think of it as looking at the world through a single lens, such as the height of individuals in a study.\nBivariate data sets step up the complexity by involving two related variables. This is akin to viewing the world through binoculars, allowing us to see relationships, like the correlation between height and weight.\nMultivariate data sets are the most intricate, involving three or more variables. This is where we truly begin to capture the multifaceted nature of the world, such as how height, weight, and diet might all interplay in a health study."
  },
  {
    "objectID": "posts/1.Probabilities/0-Introduction.html#data-source",
    "href": "posts/1.Probabilities/0-Introduction.html#data-source",
    "title": "1.0-Introduction and Basics",
    "section": "Data source",
    "text": "Data source\nThe origin of our data is equally critical, as it can influence its clarity and reliability:\n\nObservational data is akin to taking notes from the unaltered course of events. It’s uncontrolled and natural, but often messy with noise and outliers—like jotting down bird songs in a forest.\nExperimental data comes from more controlled environments where researchers actively manipulate conditions to see effects clearly. Imagine planting trees at varying elevations to measure growth differences. Here, a confounding variable, like soil type, could still influence results, even though it wasn’t directly controlled for."
  },
  {
    "objectID": "posts/1.Probabilities/0-Introduction.html#data-types",
    "href": "posts/1.Probabilities/0-Introduction.html#data-types",
    "title": "1.0-Introduction and Basics",
    "section": "Data Types",
    "text": "Data Types\nAdditionally, the kind of data we collect determines our analytical approach:\n\nCategorical data involves placing data into buckets or categories. It’s about organizing and counting, such as tallying voters by political party affiliation.\nQuantitative data refers to numerical measurements that are often continuous, like measuring rainfall in inches."
  },
  {
    "objectID": "posts/1.Probabilities/0-Introduction.html#measurement-scales",
    "href": "posts/1.Probabilities/0-Introduction.html#measurement-scales",
    "title": "1.0-Introduction and Basics",
    "section": "Measurement Scales",
    "text": "Measurement Scales\nWhen we measure, we also consider the scale:\n\nNominal scales are for simple categorization, where the order doesn’t matter—like tagging animals with unique IDs.\nOrdinal scales introduce a ranking system, where the position holds meaning, such as classifying hotel reviews from one to five stars.\nInterval scales allow for quantifying the difference between items, but lack a true zero point, like dates in a calendar.\nRatio scales give us the most detailed view, with both relative differences and a meaningful zero point, such as zero speed or zero income.\n\nUnderstanding these classifications provides a critical backdrop before stepping into the world of machine learning, where data is not just passively observed but actively used to train algorithms to make predictions and uncover insights."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I am Ahmed Aredah, a PhD student specializing in Civil Engineering with a focus on transportation. Concurrently, I’m also pursuing an MSc in Computer Science. My academic journey has equipped me with a unique blend of knowledge, bridging the gap between the worlds of transportation and computer science.\nI take pride in being the developer behind NeTrainSim, an open-source network train simulator. NeTrainSim is a collaborative effort between Virginia Tech (VT), North Carolina State University (NC), and Deutsch Bahn (DB). The project was supervised and steered by the esteemed Prof. Hesham Rakha. This initiative is a testament to my passion for contributing to the community and my commitment to advancing the field of transportation through technological innovations and collaborative research.\nWithin this blog, I will be showing the basics of machine learning techniques without going into too much of techniqal complexities. In addition, I will shed the lights on a quick overview of each technique with a little bit of some code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine_learning_basics",
    "section": "",
    "text": "1.0-Introduction and Basics\n\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nAhmed Aredah\n\n\n\n\n\n\n  \n\n\n\n\n1.1-Describe Data\n\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nAhmed Aredah\n\n\n\n\n\n\n  \n\n\n\n\n3.1-Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nAhmed Aredah\n\n\n\n\n\n\n  \n\n\n\n\n2.1-Multiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nAhmed Aredah\n\n\n\n\n\n\n  \n\n\n\n\n2.2-Support Vector Regression\n\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nAhmed Aredah\n\n\n\n\n\n\n  \n\n\n\n\n1.2-Probability Theory\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nAhmed Aredah\n\n\n\n\n\n\n  \n\n\n\n\n3.2-k-Nearest Neighbour (k-NN)\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nAhmed Aredah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/1.Probabilities/1-Describe Data.html",
    "href": "posts/1.Probabilities/1-Describe Data.html",
    "title": "1.1-Describe Data",
    "section": "",
    "text": "In the process of understanding and leveraging data for machine learning and analytical purposes, describing and summarizing the data is a pivotal step. Each type of measurement scale—nominal, ordinal, interval, and ratio—has specific statistical methods that can be used to provide a clear and concise summary of the data. These methods are crucial in extracting meaningful patterns and insights which can guide further analysis and model building."
  },
  {
    "objectID": "posts/1.Probabilities/1-Describe Data.html#nominal-scale",
    "href": "posts/1.Probabilities/1-Describe Data.html#nominal-scale",
    "title": "1.1-Describe Data",
    "section": "Nominal Scale",
    "text": "Nominal Scale\nWith data on a nominal scale, which involves categorization without an intrinsic order, we can:\n\nCount the number of cases per category to understand frequency distribution.\nDetermine the mode, which is the most frequently occurring category in the dataset.\nUse a contingency table to display the frequency distribution among multiple categories and understand the correlation between different nominal variables, especially within multivariate data sets."
  },
  {
    "objectID": "posts/1.Probabilities/1-Describe Data.html#ordinal-scale",
    "href": "posts/1.Probabilities/1-Describe Data.html#ordinal-scale",
    "title": "1.1-Describe Data",
    "section": "Ordinal Scale",
    "text": "Ordinal Scale\nOrdinal scale data is categorical data with a clear order or rank. For ordinal data, we utilize all the descriptive methods of nominal data, plus:\n\nThe median, which is the middle value when all observations are ordered from the least to the greatest.\nPercentiles, which divide the data into 100 equal parts, helping in understanding the distribution across the scale."
  },
  {
    "objectID": "posts/1.Probabilities/1-Describe Data.html#interval-and-ratio-scale",
    "href": "posts/1.Probabilities/1-Describe Data.html#interval-and-ratio-scale",
    "title": "1.1-Describe Data",
    "section": "Interval and Ratio Scale",
    "text": "Interval and Ratio Scale\nFor data on interval and ratio scales—which have ordered categories and meaningful differences between values—the following statistical descriptors are used:\n\nThe mean offers a measure of central tendency, providing an average value.\nThe standard deviation measures the amount of variation or dispersion in a set of values.\nCorrelation coefficients can illustrate the strength and direction of the relationship between multiple variables in multivariate data.\nThe relative standard deviation or coefficient of variation is specific to the ratio scale and is used to standardize the standard deviation relative to the mean, offering a dimensionless measure of variability.\n\nUnderstanding these statistical descriptors and their appropriate application is essential for anyone venturing into data analysis. By accurately describing data, analysts and machine learning practitioners can make well-informed decisions about the types of analysis that are most appropriate for their data and can begin to discern the stories that the data are telling."
  },
  {
    "objectID": "posts/2.Regression/1-LinearRegression.html",
    "href": "posts/2.Regression/1-LinearRegression.html",
    "title": "2.1-Multiple Linear Regression",
    "section": "",
    "text": "When modeling the relationship between a dependent variable y and multiple independent variables x_1, x_2, \\dots, x_n, the linear equation can be written as: \\begin{equation}\ny_i=\\beta_0+\\beta_1 x_{i 1}+\\beta_2 x_{i 2}+\\cdots+\\beta_n x_{i n}+\\epsilon_i\n\\end{equation}\nwhere:\n\ny_i is the observed value of the dependent variable for the i^{th} observation.\nx_1, x_2, \\dots, x_n are the values of the independent variables for the i^{th} observation.\n\\beta_0, \\beta_1, \\beta_n are the regression coefficients, with \\beta_0 being the y-intercept.\n\\epsilon_i is the error term for the i^{th} observation, capturing the difference between the observed value and the value predicted by the model.\n\nThe requirement here is to find the \\beta_i that minimizes the meas square error of \\epsilon_i. The above could be solved in multiple ways. However, one easy way to solve it is through matrix multiplication since CPU’s can deal faster with matrix manipulation.\n\n\nGiven:\n\n\\boldsymbol{X} is the design matrix of size m \\times (n + 1).\n\\boldsymbol{y} is a column vector of size m \\times 1 containing the dependent variable values.\n\\boldsymbol{\\beta} is a column vector of size (n + 1) \\times 1 containing the regression coefficients.\n\\boldsymbol{\\epsilon} is a column vector of size m \\times 1 representing the errors.\n\nThe relationship is given by: \\begin{equation}\n\\large{y} = \\large{X} \\beta + \\epsilon\n\\end{equation}\nTo determine the value of \\beta, one might think to rearrange the equation as:\n\\begin{equation}\n\\beta = \\frac{\\large{y}}{\\large{X}}\n\\end{equation}\nHowever, this representation is not accurate in the context of matrix operations. The inaccuracy arises due to the nature of matrices and how they are manipulated. Here’s a breakdown of the issues:\n\nMatrix Division: In the realm of matrices, there isn’t a direct concept of division like there is with regular numbers. So, saying \\beta = \\frac{y}{X} doesn’t have a straightforward meaning.\nMatrix Multiplication: Matrix multiplication is not commutative. This means that the product AB is not necessarily the same as the product BA. So, even if we were to try to “isolate” \\beta by some matrix operation, it wouldn’t be as simple as dividing both sides by X.\nCorrect Approach: The correct way to “isolate” \\beta when dealing with matrices is to multiply both sides of the equation by the inverse of X (assuming X is invertible). The equation would look something like: \\beta = X^{-1}y. Note that this equation assumes that X is a square matrix and has an inverse. If X is not square, or doesn’t have an inverse, other methods like the Moore-Penrose pseudoinverse would be used to estimate \\beta.\nDimensionality: Even if we were to entertain the idea of matrix division, the dimensions must be compatible. In the equation y = X\\beta + \\epsilon, y is a column vector of size m \\times 1, X is a matrix of size m \\times n, and \\beta is a column vector of size n \\times 1. Dividing an m \\times 1 vector by an m \\times n matrix doesn’t produce a consistent result in terms of matrix dimensions.\n\nThe reason we use the equation $ = (^T )^{-1} ^T $ instead of $ = ^{-1} $ is due to the structure and properties of the design matrix ( ) in linear regression.\n\nNon-Square Matrix: In most real-world applications of linear regression, \\mathbf{X} is not a square matrix. It usually has more rows (observations) than columns (predictors). Only square matrices possess inverses in the traditional sense. Therefore, \\mathbf{X}^{-1} doesn’t exist for these cases.\nPseudo-Inverse: The expression (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T is known as the Moore-Penrose pseudo-inverse of \\mathbf{X}. This pseudo-inverse provides a means to approximate an inverse.\nProjection onto Column Space: The term \\mathbf{X}^T \\mathbf{y} can be interpreted as projecting the response vector \\mathbf{y} onto the column space of \\mathbf{X}.\nMinimization of Residuals: The expression (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T originates from differentiating the sum of squared residuals with respect to \\boldsymbol{\\beta} and setting it to zero.\n\nFinally, what we are trying to accomplish is to reduce the error between the model and the actual data: \nWe’ll be applying this concept to the cars dataset."
  },
  {
    "objectID": "posts/2.Regression/1-LinearRegression.html#mathematical-formulation",
    "href": "posts/2.Regression/1-LinearRegression.html#mathematical-formulation",
    "title": "2.1-Multiple Linear Regression",
    "section": "",
    "text": "When modeling the relationship between a dependent variable y and multiple independent variables x_1, x_2, \\dots, x_n, the linear equation can be written as: \\begin{equation}\ny_i=\\beta_0+\\beta_1 x_{i 1}+\\beta_2 x_{i 2}+\\cdots+\\beta_n x_{i n}+\\epsilon_i\n\\end{equation}\nwhere:\n\ny_i is the observed value of the dependent variable for the i^{th} observation.\nx_1, x_2, \\dots, x_n are the values of the independent variables for the i^{th} observation.\n\\beta_0, \\beta_1, \\beta_n are the regression coefficients, with \\beta_0 being the y-intercept.\n\\epsilon_i is the error term for the i^{th} observation, capturing the difference between the observed value and the value predicted by the model.\n\nThe requirement here is to find the \\beta_i that minimizes the meas square error of \\epsilon_i. The above could be solved in multiple ways. However, one easy way to solve it is through matrix multiplication since CPU’s can deal faster with matrix manipulation.\n\n\nGiven:\n\n\\boldsymbol{X} is the design matrix of size m \\times (n + 1).\n\\boldsymbol{y} is a column vector of size m \\times 1 containing the dependent variable values.\n\\boldsymbol{\\beta} is a column vector of size (n + 1) \\times 1 containing the regression coefficients.\n\\boldsymbol{\\epsilon} is a column vector of size m \\times 1 representing the errors.\n\nThe relationship is given by: \\begin{equation}\n\\large{y} = \\large{X} \\beta + \\epsilon\n\\end{equation}\nTo determine the value of \\beta, one might think to rearrange the equation as:\n\\begin{equation}\n\\beta = \\frac{\\large{y}}{\\large{X}}\n\\end{equation}\nHowever, this representation is not accurate in the context of matrix operations. The inaccuracy arises due to the nature of matrices and how they are manipulated. Here’s a breakdown of the issues:\n\nMatrix Division: In the realm of matrices, there isn’t a direct concept of division like there is with regular numbers. So, saying \\beta = \\frac{y}{X} doesn’t have a straightforward meaning.\nMatrix Multiplication: Matrix multiplication is not commutative. This means that the product AB is not necessarily the same as the product BA. So, even if we were to try to “isolate” \\beta by some matrix operation, it wouldn’t be as simple as dividing both sides by X.\nCorrect Approach: The correct way to “isolate” \\beta when dealing with matrices is to multiply both sides of the equation by the inverse of X (assuming X is invertible). The equation would look something like: \\beta = X^{-1}y. Note that this equation assumes that X is a square matrix and has an inverse. If X is not square, or doesn’t have an inverse, other methods like the Moore-Penrose pseudoinverse would be used to estimate \\beta.\nDimensionality: Even if we were to entertain the idea of matrix division, the dimensions must be compatible. In the equation y = X\\beta + \\epsilon, y is a column vector of size m \\times 1, X is a matrix of size m \\times n, and \\beta is a column vector of size n \\times 1. Dividing an m \\times 1 vector by an m \\times n matrix doesn’t produce a consistent result in terms of matrix dimensions.\n\nThe reason we use the equation $ = (^T )^{-1} ^T $ instead of $ = ^{-1} $ is due to the structure and properties of the design matrix ( ) in linear regression.\n\nNon-Square Matrix: In most real-world applications of linear regression, \\mathbf{X} is not a square matrix. It usually has more rows (observations) than columns (predictors). Only square matrices possess inverses in the traditional sense. Therefore, \\mathbf{X}^{-1} doesn’t exist for these cases.\nPseudo-Inverse: The expression (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T is known as the Moore-Penrose pseudo-inverse of \\mathbf{X}. This pseudo-inverse provides a means to approximate an inverse.\nProjection onto Column Space: The term \\mathbf{X}^T \\mathbf{y} can be interpreted as projecting the response vector \\mathbf{y} onto the column space of \\mathbf{X}.\nMinimization of Residuals: The expression (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T originates from differentiating the sum of squared residuals with respect to \\boldsymbol{\\beta} and setting it to zero.\n\nFinally, what we are trying to accomplish is to reduce the error between the model and the actual data: \nWe’ll be applying this concept to the cars dataset."
  },
  {
    "objectID": "posts/2.Regression/1-LinearRegression.html#application-on-cars-selling-dataset",
    "href": "posts/2.Regression/1-LinearRegression.html#application-on-cars-selling-dataset",
    "title": "2.1-Multiple Linear Regression",
    "section": "Application on Cars selling Dataset",
    "text": "Application on Cars selling Dataset\n\n#import required libraries\nimport pandas as pd\nimport os\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#inline plots\n%matplotlib inline\n\n# Set Seaborn style to \"whitegrid\" for a white background with grid lines\nsns.set_style(\"whitegrid\")\n\n#supress warnings!\nwarnings.simplefilter(action='ignore', category=Warning)\n# Set the display option to show all columns\npd.set_option('display.max_columns', None)\n\nprint(\"required libraries loaded successfully!\")\n\nrequired libraries loaded successfully!\n\n\nThe dataset source is accessible from here\n\n# Load the cars dataset\ncars = pd.read_csv(\"https://raw.githubusercontent.com/AhmedAredah/MachineLearningBasics/main/data/cars.csv\")\n\ncars.describe()\n\n\n\n\n\n\n\n\nyear\nselling_price\nkm_driven\nseats\n\n\n\n\ncount\n8128.000000\n8.128000e+03\n8.128000e+03\n7907.000000\n\n\nmean\n2013.804011\n6.382718e+05\n6.981951e+04\n5.416719\n\n\nstd\n4.044249\n8.062534e+05\n5.655055e+04\n0.959588\n\n\nmin\n1983.000000\n2.999900e+04\n1.000000e+00\n2.000000\n\n\n25%\n2011.000000\n2.549990e+05\n3.500000e+04\n5.000000\n\n\n50%\n2015.000000\n4.500000e+05\n6.000000e+04\n5.000000\n\n\n75%\n2017.000000\n6.750000e+05\n9.800000e+04\n5.000000\n\n\nmax\n2020.000000\n1.000000e+07\n2.360457e+06\n14.000000\n\n\n\n\n\n\n\nThe describe method in pandas provides a summary of the central tendency, dispersion, and shape of the distribution of a dataset. It returns a DataFrame that shows various descriptive statistics.\nHere’s what each row in the output means:\n\ncount: The number of non-missing values for each variable. In this case, each variable has 342 non-missing values.\nmean: The average value of each variable.\nstd: The standard deviation of each variable, which measures the amount of variation or dispersion.\nmin: The minimum value of each variable.\n25%: The 25th percentile value of each variable.\n50%: The 50th percentile value (or median) of each variable.\n75%: The 75th percentile value of each variable.\nmax: The maximum value of each variable.\n\nIn the context of linear regression, these descriptive statistics can help you understand the distribution of your variables and guide your data preprocessing steps.\nOne common issue to address during data preprocessing is the handling of missing values (NA values). There are several ways to deal with missing values:\n\nRemove rows with missing values: This is the simplest approach, but it may result in loss of valuable data. python     data = data.dropna()\nReplace missing values: You can replace missing values with a specific value, such as the mean or median of the variable. python     data['variable'].fillna(data['variable'].mean(), inplace=True)\nUse predictive imputation: This involves using other variables in the dataset to predict and fill in missing values. This can be done using machine learning algorithms or other statistical methods.\n\nBefore fitting a linear regression model, it’s important to check for outliers, as they can have a significant impact on your model. The min and max values in the describe output can help you identify any extreme values that might be outliers. You may also want to plot your data to visually inspect for outliers.\nAdditionally, the mean and std values can be used to standardize your variables, which is a common preprocessing step for linear regression. Standardizing your variables can make it easier to interpret the coefficients of your linear regression model, especially when your variables are on different scales.\nIn this case, We will just remove rows with missing values since it is easier but this could have a huge impact on the dataset.\n\n# Drop rows where any cell contains NA or NAN \ncars = cars.dropna()\n\n# show the top 5 rows\ncars.head()\n\n\n\n\n\n\n\n\nname\nyear\nselling_price\nkm_driven\nfuel\nseller_type\ntransmission\nowner\nmileage\nengine\nmax_power\ntorque\nseats\n\n\n\n\n0\nMaruti Swift Dzire VDI\n2014\n450000\n145500\nDiesel\nIndividual\nManual\nFirst Owner\n23.4 kmpl\n1248 CC\n74 bhp\n190Nm@ 2000rpm\n5.0\n\n\n1\nSkoda Rapid 1.5 TDI Ambition\n2014\n370000\n120000\nDiesel\nIndividual\nManual\nSecond Owner\n21.14 kmpl\n1498 CC\n103.52 bhp\n250Nm@ 1500-2500rpm\n5.0\n\n\n2\nHonda City 2017-2020 EXi\n2006\n158000\n140000\nPetrol\nIndividual\nManual\nThird Owner\n17.7 kmpl\n1497 CC\n78 bhp\n12.7@ 2,700(kgm@ rpm)\n5.0\n\n\n3\nHyundai i20 Sportz Diesel\n2010\n225000\n127000\nDiesel\nIndividual\nManual\nFirst Owner\n23.0 kmpl\n1396 CC\n90 bhp\n22.4 kgm at 1750-2750rpm\n5.0\n\n\n4\nMaruti Swift VXI BSIII\n2007\n130000\n120000\nPetrol\nIndividual\nManual\nFirst Owner\n16.1 kmpl\n1298 CC\n88.2 bhp\n11.5@ 4,500(kgm@ rpm)\n5.0\n\n\n\n\n\n\n\nFrom the above table we need to clean the dataset first and remove strings from columns ‘mileages’, ‘engine’, ‘max_power’, and ‘torque’\n\n# List of columns to process\ncolumns_to_process = ['mileage', 'engine', 'max_power']\n\nfor column in columns_to_process:\n    # Convert the column to string type\n    cars[column] = cars[column].astype(str)\n    \n    # Extract the first numerical value (assumes format is \"value unit\")\n    cars[column] = cars[column].str.split().str[0]\n    \n    # Convert those values to float, set others to NaN if they can't be converted\n    cars[column] = pd.to_numeric(cars[column], errors='coerce')\n\n\nimport re\n\n# Function to extract the numeric part before 'Nm'\ndef extract_torque_value(s):\n    match = re.search(r'(\\d+)Nm', s)\n    return float(match.group(1)) if match else None\n\n# Apply the function to the torque column\ncars['torque'] = cars['torque'].apply(extract_torque_value)\ncars.dropna()\n\n\n\n\n\n\n\n\nname\nyear\nselling_price\nkm_driven\nfuel\nseller_type\ntransmission\nowner\nmileage\nengine\nmax_power\ntorque\nseats\n\n\n\n\n0\nMaruti Swift Dzire VDI\n2014\n450000\n145500\nDiesel\nIndividual\nManual\nFirst Owner\n23.40\n1248\n74.00\n190.0\n5.0\n\n\n1\nSkoda Rapid 1.5 TDI Ambition\n2014\n370000\n120000\nDiesel\nIndividual\nManual\nSecond Owner\n21.14\n1498\n103.52\n250.0\n5.0\n\n\n7\nMaruti 800 DX BSII\n2001\n45000\n5000\nPetrol\nIndividual\nManual\nSecond Owner\n16.10\n796\n37.00\n59.0\n4.0\n\n\n8\nToyota Etios VXD\n2011\n350000\n90000\nDiesel\nIndividual\nManual\nFirst Owner\n23.59\n1364\n67.10\n170.0\n5.0\n\n\n9\nFord Figo Diesel Celebration Edition\n2013\n200000\n169000\nDiesel\nIndividual\nManual\nFirst Owner\n20.00\n1399\n68.10\n160.0\n5.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8122\nHyundai i20 Magna 1.4 CRDi\n2014\n475000\n80000\nDiesel\nIndividual\nManual\nSecond Owner\n22.54\n1396\n88.73\n7.0\n5.0\n\n\n8123\nHyundai i20 Magna\n2013\n320000\n110000\nPetrol\nIndividual\nManual\nFirst Owner\n18.50\n1197\n82.85\n7.0\n5.0\n\n\n8125\nMaruti Swift Dzire ZDi\n2009\n382000\n120000\nDiesel\nIndividual\nManual\nFirst Owner\n19.30\n1248\n73.90\n190.0\n5.0\n\n\n8126\nTata Indigo CR4\n2013\n290000\n25000\nDiesel\nIndividual\nManual\nFirst Owner\n23.57\n1396\n70.00\n140.0\n5.0\n\n\n8127\nTata Indigo CR4\n2013\n290000\n25000\nDiesel\nIndividual\nManual\nFirst Owner\n23.57\n1396\n70.00\n140.0\n5.0\n\n\n\n\n7033 rows × 13 columns\n\n\n\nBefore diving into machine learning, understanding the descriptive statistics of our data is crucial. It provides insights into the distribution, tendencies, and range of our data. This preliminary step ensures that we’re aware of the data’s characteristics, helping in making informed decisions about preprocessing, model selection, and interpretation of results. Such an understanding can aid in identifying anomalies, ensuring data quality, and setting the right expectations from the model’s predictions.\n\ncars.describe()\n\n\n\n\n\n\n\n\nyear\nselling_price\nkm_driven\nmileage\nengine\nmax_power\ntorque\nseats\n\n\n\n\ncount\n7906.000000\n7.906000e+03\n7.906000e+03\n7906.000000\n7906.000000\n7906.000000\n7033.000000\n7906.000000\n\n\nmean\n2013.983936\n6.498137e+05\n6.918866e+04\n19.419861\n1458.708829\n91.587374\n158.266032\n5.416393\n\n\nstd\n3.863695\n8.135827e+05\n5.679230e+04\n4.036263\n503.893057\n35.747216\n107.169575\n0.959208\n\n\nmin\n1994.000000\n2.999900e+04\n1.000000e+00\n0.000000\n624.000000\n32.800000\n1.000000\n2.000000\n\n\n25%\n2012.000000\n2.700000e+05\n3.500000e+04\n16.780000\n1197.000000\n68.050000\n90.000000\n5.000000\n\n\n50%\n2015.000000\n4.500000e+05\n6.000000e+04\n19.300000\n1248.000000\n82.000000\n146.000000\n5.000000\n\n\n75%\n2017.000000\n6.900000e+05\n9.542500e+04\n22.320000\n1582.000000\n102.000000\n200.000000\n5.000000\n\n\nmax\n2020.000000\n1.000000e+07\n2.360457e+06\n42.000000\n3604.000000\n400.000000\n789.000000\n14.000000\n\n\n\n\n\n\n\n\n# Set up the matplotlib figure\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\n\n# Plot the distribution of each variable\nhist1 = sns.histplot(data=cars, x='year', kde=True, ax=axes[0, 0], color='skyblue')\nhist2 = sns.histplot(data=cars, x='selling_price', kde=True, ax=axes[0, 1], color='skyblue')\nhist3 = sns.histplot(data=cars, x='km_driven', kde=True, ax=axes[0, 2], color='skyblue')\nhist4 = sns.histplot(data=cars, x='engine', kde=True, ax=axes[0, 3], color='skyblue')\nhist5 = sns.histplot(data=cars, x='max_power', kde=True, ax=axes[1, 0], color='skyblue')\nhist6 = sns.histplot(data=cars, x='mileage', kde=True, ax=axes[1, 1], color='skyblue')\nhist7 = sns.histplot(data=cars, x='torque', kde=True, ax=axes[1, 2], color='skyblue')\n\n# Plot the count of each category for categorical variables\nseats_plot = sns.countplot(data=cars, x='seats', ax=axes[1, 3])\nfuel_plot = sns.countplot(data=cars, x='fuel', ax=axes[2, 0])\ntransmission_plot = sns.countplot(data=cars, x='transmission', ax=axes[2, 1])\nowner_plot = sns.countplot(data=cars, x='owner', ax=axes[2, 2])\nseller_plot = sns.countplot(data=cars, x='seller_type', ax=axes[2, 3])\n\n# Make the x-axis text vertical for all plots\nfor ax in axes.flatten():\n    plt.sca(ax)\n    plt.xticks(rotation=45)\n\n# Adjust the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\nWhen applying linear regression to this dataset, it’s crucial to consider these distributions. The insights derived from these plots can help in feature selection, outlier detection, and in understanding the relationships between variables. For instance, the dominance of diesel and petrol cars might mean that other fuel types have less influence on the selling price. Similarly, the large number of manual transmission cars might imply that automatic transmission could be a premium feature, potentially impacting the price.\nfrom the above distribution we find that selling price, and km_driven are positively skewed and we need to apply a transformation function to make it normally distributed.\n\nimport numpy as np\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\ntransformed_data = np.log1p(cars['selling_price'])  # log1p helps in dealing with zero values in the original data\ntransformed_data_2 = np.log1p(cars['km_driven'])\nsns.histplot(transformed_data, ax=axes[0]);\nsns.histplot(transformed_data_2, ax=axes[1]);\n\n\n\n\nWe will compare the regression model with and without the transformation\n\n# Calculate VIF for each predictor variable\ncars_numeric = cars.select_dtypes(include='number')\ncars_numeric = cars_numeric.fillna(cars_numeric.mean())\n\nvif_data = pd.DataFrame()\nvif_data[\"variable\"] = cars_numeric.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(cars_numeric.values, i) for i in range(cars_numeric.shape[1])]\nvif_data.head(n=10)\n\n\n\n\n\n\n\n\nvariable\nVIF\n\n\n\n\n0\nyear\n140.187417\n\n\n1\nselling_price\n4.337223\n\n\n2\nkm_driven\n2.993602\n\n\n3\nmileage\n40.354886\n\n\n4\nengine\n41.552858\n\n\n5\nmax_power\n34.167509\n\n\n6\ntorque\n7.961410\n\n\n7\nseats\n66.993390\n\n\n\n\n\n\n\nVariance Inflation Factor (VIF) is a measure that helps to identify multicollinearity in regression models. When interpreting the VIF, a general rule of thumb is that a VIF above 5-10 suggests a problematic amount of collinearity. Given the high VIF values for ‘Year’, ‘mileage’, ‘engine’, ‘max_power’, and ‘Seats’, one should consider further analysis or remedial measures to address potential multicollinearity before proceeding with building a regression model.\nFrom this, I will drop the highest 2 VIF column value and redo the analysis again.\n\ncars_numeric = cars_numeric.drop([\"year\", \"seats\", \"max_power\"], axis=1)\n\n\nvif_data = pd.DataFrame()\nvif_data[\"variable\"] = cars_numeric.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(cars_numeric.values, i) for i in range(cars_numeric.shape[1])]\nvif_data.head(n=10)\n\n\n\n\n\n\n\n\nvariable\nVIF\n\n\n\n\n0\nselling_price\n2.776218\n\n\n1\nkm_driven\n2.982095\n\n\n2\nmileage\n4.839869\n\n\n3\nengine\n12.135908\n\n\n4\ntorque\n7.028583\n\n\n\n\n\n\n\nThe resultant VIF indicates almost no multicolinearity in the data.\nBefore we proceed, let’s drop the columns that we decided to drop before in the VIF analysis.\n\ncars = cars.drop([\"year\", \"seats\", \"max_power\"], axis=1)\n\nFor the categorical data, the regression model does not understand strings (text) so we need to find a way to transfer this text in numbers. one way is to encode them which converts the categories into a series of 1,2,3 coresponding to the categorical order. OneHoteEncder does this job for us.\n\n# Preprocessing: One-hot encode categorical variables\ncategorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']\none_hot = OneHotEncoder(drop='first', sparse=False)  # drop='first' to avoid multicollinearity\nencoded_features = one_hot.fit_transform(cars[categorical_columns])\nencoded_df = pd.DataFrame(encoded_features, columns=one_hot.get_feature_names_out(categorical_columns))\n\n\n# Concatenate encoded features with the original dataframe\ncars = pd.concat([cars, encoded_df], axis=1)\n\n# Drop the original categorical columns and other non-numeric columns\ncars = cars.drop(columns=categorical_columns + ['name', 'torque'])\n\nAgain, let’s make sure we dont have any missing values before we continue by filling with the mean value.\n\n# Define predictors (X) and target variable (y)\nX = cars.drop('selling_price', axis=1)\ny = cars['selling_price']\nX = X.fillna(X.mean())\ny = y.fillna(y.mean())\n\nWe will do the regression model here.\n\nFirst regression model without transformation\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the Linear Regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = lin_reg.predict(X_test)\n\n# Evaluate the model's performance\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 (coefficient of determination): {r2:.2f}\")\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse:.2f}\")\n\n# Model's coefficients and intercept\nprint(f\"Intercept: {lin_reg.intercept_}\")\nprint(f\"Coefficients: {lin_reg.coef_}\")\n\nR^2 (coefficient of determination): 0.37\nMean Squared Error: 419706652720.62\nIntercept: -960727.9684812507\nCoefficients: [-4.21035185e+00  3.33345338e+04  9.85201248e+02 -1.36989350e+05\n -2.04315878e+05 -1.37114189e+05 -4.11308191e+04  1.61218399e+05\n -7.12835842e+03 -1.31152299e+05 -2.25614271e+04  9.90031360e+05\n -3.43569635e+04]\n\n\n\n# Selecting the first feature for demonstration\nfeature_index = 0\nfeature_name = X.columns[feature_index]\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of actual vs. predicted\nplt.scatter(X_test[feature_name], y_test, color='skyblue', label='Actual Values')\nplt.scatter(X_test[feature_name], y_pred, color='red', marker='x', label='Predicted Values')\n\n\nplt.title('Regression Fit for Feature: ' + feature_name)\nplt.xlabel(feature_name)\nplt.ylabel('Selling Price')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n# Calculate residuals\nresiduals = y_test - y_pred\n\n# Plot residuals\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.title('Residuals vs. Predicted Values')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\nThe error plot indicates the error distribution is not random and there is a trend in the error that the model could not capture. Let’s then try the second regression model where we incorporate the transformation.\n\n\n\nSecond regression model with transformation\nnow, lets transform the selling price and km_driven columns using log() function and create a new regression model\n\ncars['selling_price'] = np.log1p(cars['selling_price'])\ncars['km_driven'] = np.log1p(cars['km_driven'])\n\n\n# Define predictors (X) and target variable (y)\nX = cars.drop('selling_price', axis=1)\ny = cars['selling_price']\nX_train = X_train.dropna()\ny_train = y_train[X_train.index]  # Keep corresponding y values\n\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Drop rows with NaN values from X_train\nX_train = X_train.dropna()\n# Synchronize y_train with the updated X_train\ny_train = y_train.loc[X_train.index]\nX_test = X_test.dropna()\ny_test = y_test.loc[X_test.index]\n\n\n# Create and train the Linear Regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = lin_reg.predict(X_test)\n\n# Evaluate the model's performance\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 (coefficient of determination): {r2:.2f}\")\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse:.2f}\")\n\n# Model's coefficients and intercept\nprint(f\"Intercept: {lin_reg.intercept_}\")\nprint(f\"Coefficients: {lin_reg.coef_}\")\n\nR^2 (coefficient of determination): 0.60\nMean Squared Error: 0.27\nIntercept: 14.408622302677129\nCoefficients: [-4.35206639e-01  7.52823926e-02  1.32585312e-03  1.64254578e-04\n -9.24979659e-02 -1.18337658e-02 -6.49937878e-02  4.32081971e-02\n -1.68184483e-02 -7.34963940e-02 -1.12130018e-02 -7.46682688e-01\n -4.84720247e-02]\n\n\n\n# Selecting the first feature for demonstration\nfeature_index = 0\nfeature_name = X.columns[feature_index]\n\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of actual vs. predicted\nplt.scatter(X_test[feature_name], y_test, color='skyblue', label='Actual Values')\nplt.scatter(X_test[feature_name], y_pred, color='red', marker='x', label='Predicted Values')\n\n\nplt.title('Regression Fit for Feature: ' + feature_name)\nplt.xlabel(feature_name)\nplt.ylabel('Selling Price')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n# Calculate residuals\nresiduals = y_test - y_pred\n\n# Plot residuals\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.title('Residuals vs. Predicted Values')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\nHere, the error plot indicates the error is random and does not show any trend in the data. Thus the model we have is much better than the previous model.\nby doing the transformation, we gained around 23% in the fit.\nHence the final equation is:\n\\begin{equation}\n\\begin{split}\n\\text{selling\\_price} = & e^{14.4086} \\\\\n& \\times e^{-0.4352 \\times \\log(\\text{km\\_driven})} \\\\\n& \\times e^{0.0753 \\times \\text{mileage}} \\\\\n& \\times e^{0.0013 \\times \\text{engine}} \\\\\n& \\times e^{0.0002 \\times \\text{fuel\\_Diesel}} \\\\\n& \\times e^{-0.0925 \\times \\text{fuel\\_LPG}} \\\\\n& \\times e^{-0.0118 \\times \\text{fuel\\_Petrol}} \\\\\n& \\times e^{-0.0650 \\times \\text{seller\\_type\\_Individual}} \\\\\n& \\times e^{0.0432 \\times \\text{seller\\_type\\_Trustmark Dealer}} \\\\\n& \\times e^{-0.0168 \\times \\text{transmission\\_Manual}} \\\\\n& \\times e^{-0.0735 \\times \\text{owner\\_Fourth \\& Above Owner}} \\\\\n& \\times e^{-0.0112 \\times \\text{owner\\_Second Owner}} \\\\\n& \\times e^{-0.7467 \\times \\text{owner\\_Test Drive Car}} \\\\\n& \\times e^{-0.0485 \\times \\text{owner\\_Third Owner}}\n\\end{split}\n\\end{equation}"
  },
  {
    "objectID": "posts/3.Classification/1-LogisticRegression.html",
    "href": "posts/3.Classification/1-LogisticRegression.html",
    "title": "3.1-Logistic Regression",
    "section": "",
    "text": "When modeling the relationship between a dependent variable y and multiple independent variables x_1, x_2, \\dots, x_n, the logistic function can be written as:\n\\begin{equation}\ny_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_n x_{in})}}\n\\end{equation}\nwhere:\n\ny_i is the observed value of the dependent variable for the i^{th} observation.\nx_1, x_2, \\dots, x_n are the values of the independent variables for the i^{th} observation.\n\\beta_0, \\beta_1, \\beta_n are the regression coefficients, with \\beta_0 being the y-intercept.\n\nThe goal of logistic regression is to find the values of \\beta_0, \\beta_1, \\dots, \\beta_n that maximize the likelihood of the observed data. This is often done using a method known as maximum likelihood estimation (MLE).\n\n\nGiven:\n\n\\boldsymbol{X} is the design matrix of size m \\times (n + 1).\n\\boldsymbol{y} is a column vector of size m \\times 1 containing the dependent variable values.\n\\boldsymbol{\\beta} is a column vector of size (n + 1) \\times 1 containing the regression coefficients.\n\\boldsymbol{\\epsilon} is a column vector of size m \\times 1 representing the errors.\n\nThe relationship can be represented as:\n\n\\large{y} = \\frac{1}{1 + e^{-(\\mathbf{X}\\boldsymbol{\\beta})}}\n\nThe likelihood of the observed data given the model parameters can be written as:\n\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^m p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta})\n\nwhere p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}) is the probability of observing y_i given the independent variables \\mathbf{x}_i and the model parameters \\boldsymbol{\\beta}. The goal of MLE is to find the values of \\boldsymbol{\\beta} that maximize L(\\boldsymbol{\\beta}).\n\n\n\nSince the likelihood L(\\boldsymbol{\\beta}) is a product of many small numbers, it can be more convenient to work with the log likelihood, which is the natural logarithm of the likelihood:\n\n\\log L(\\boldsymbol{\\beta}) = \\sum_{i=1}^m \\log p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta})\n\nThe log likelihood can be maximized using various optimization algorithms, such as gradient ascent or Newton’s method."
  },
  {
    "objectID": "posts/3.Classification/1-LogisticRegression.html#mathematical-formulation",
    "href": "posts/3.Classification/1-LogisticRegression.html#mathematical-formulation",
    "title": "3.1-Logistic Regression",
    "section": "",
    "text": "When modeling the relationship between a dependent variable y and multiple independent variables x_1, x_2, \\dots, x_n, the logistic function can be written as:\n\\begin{equation}\ny_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_n x_{in})}}\n\\end{equation}\nwhere:\n\ny_i is the observed value of the dependent variable for the i^{th} observation.\nx_1, x_2, \\dots, x_n are the values of the independent variables for the i^{th} observation.\n\\beta_0, \\beta_1, \\beta_n are the regression coefficients, with \\beta_0 being the y-intercept.\n\nThe goal of logistic regression is to find the values of \\beta_0, \\beta_1, \\dots, \\beta_n that maximize the likelihood of the observed data. This is often done using a method known as maximum likelihood estimation (MLE).\n\n\nGiven:\n\n\\boldsymbol{X} is the design matrix of size m \\times (n + 1).\n\\boldsymbol{y} is a column vector of size m \\times 1 containing the dependent variable values.\n\\boldsymbol{\\beta} is a column vector of size (n + 1) \\times 1 containing the regression coefficients.\n\\boldsymbol{\\epsilon} is a column vector of size m \\times 1 representing the errors.\n\nThe relationship can be represented as:\n\n\\large{y} = \\frac{1}{1 + e^{-(\\mathbf{X}\\boldsymbol{\\beta})}}\n\nThe likelihood of the observed data given the model parameters can be written as:\n\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^m p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta})\n\nwhere p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}) is the probability of observing y_i given the independent variables \\mathbf{x}_i and the model parameters \\boldsymbol{\\beta}. The goal of MLE is to find the values of \\boldsymbol{\\beta} that maximize L(\\boldsymbol{\\beta}).\n\n\n\nSince the likelihood L(\\boldsymbol{\\beta}) is a product of many small numbers, it can be more convenient to work with the log likelihood, which is the natural logarithm of the likelihood:\n\n\\log L(\\boldsymbol{\\beta}) = \\sum_{i=1}^m \\log p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta})\n\nThe log likelihood can be maximized using various optimization algorithms, such as gradient ascent or Newton’s method."
  },
  {
    "objectID": "posts/3.Classification/1-LogisticRegression.html#application-on-the-breast-cancer-wisconsin-dataset",
    "href": "posts/3.Classification/1-LogisticRegression.html#application-on-the-breast-cancer-wisconsin-dataset",
    "title": "3.1-Logistic Regression",
    "section": "Application on the Breast Cancer Wisconsin Dataset",
    "text": "Application on the Breast Cancer Wisconsin Dataset\nNow that we have an understanding of logistic regression, let’s apply these concepts to the Breast Cancer Wisconsin dataset. This dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, and the goal is to classify the mass as benign or malignant. Stay tuned for a deep dive into the analysis and insights we can extract from this dataset!\n\n#import required libraries\nimport pandas as pd\nimport os\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\n\n\n#inline plots\n%matplotlib inline\n\n# Set Seaborn style to \"whitegrid\" for a white background with grid lines\nsns.set_style(\"whitegrid\")\n\n#supress warnings!\nwarnings.simplefilter(action='ignore', category=Warning)\n# Set the display option to show all columns\npd.set_option('display.max_columns', None)\n\nprint(\"required libraries loaded successfully!\")\n\nrequired libraries loaded successfully!\n\n\n\n# Load the dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nfeature_names = data.feature_names\n\n# Convert the data to a pandas DataFrame\ndf = pd.DataFrame(X, columns=feature_names)\ndf['target'] = y\n\ndf.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\nradius error\ntexture error\nperimeter error\narea error\nsmoothness error\ncompactness error\nconcavity error\nconcave points error\nsymmetry error\nfractal dimension error\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n1.0950\n0.9053\n8.589\n153.40\n0.006399\n0.04904\n0.05373\n0.01587\n0.03003\n0.006193\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n0.5435\n0.7339\n3.398\n74.08\n0.005225\n0.01308\n0.01860\n0.01340\n0.01389\n0.003532\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n0.7456\n0.7869\n4.585\n94.03\n0.006150\n0.04006\n0.03832\n0.02058\n0.02250\n0.004571\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n0.4956\n1.1560\n3.445\n27.23\n0.009110\n0.07458\n0.05661\n0.01867\n0.05963\n0.009208\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n0.7572\n0.7813\n5.438\n94.44\n0.011490\n0.02461\n0.05688\n0.01885\n0.01756\n0.005115\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0\n\n\n\n\n\n\n\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the classifier\nclf = LogisticRegression(random_state=42, max_iter=10000)\n\n# Perform 5-fold cross-validation\ncv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n\n# Print the cross-validation scores\nprint(\"Cross-validation scores:\", cv_scores)\n\n# Print the average cross-validation score\nprint(\"Average cross-validation score:\", cv_scores.mean())\n\n# Train the classifier on the training data\nclf.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = clf.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nCross-validation scores: [0.95604396 0.95604396 0.96703297 0.96703297 0.92307692]\nAverage cross-validation score: 0.9538461538461538\nAccuracy: 0.956140350877193\n\n\n\n# Plot the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\nThe given confusion matrix represents the performance of a classification model on the breast cancer dataset from the sklearn library.\nFrom the matrix:\nThe model correctly predicted 39 instances as class 0 (True Negative) and 70 instances as class 1 (True Positive). There were 4 instances where the model incorrectly predicted class 1 when the actual class was 0 (False Positive). Similarly, there was 1 instance where the model incorrectly predicted class 0 when the actual class was 1 (False Negative). In summary, the model has a high number of true predictions and a low number of false predictions, indicating a good classification performance for this dataset.\nlet’s reduce the x dimensionality to be able to visualize it on a simple plot\n\n# Reduce the dimensionality of the dataset to 2 dimensions using PCA so we can visualize it\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n\n# Initialize the classifier\nclf = LogisticRegression(random_state=42, max_iter=10000)\n\n# Train the classifier on the training data\nclf.fit(X_train, y_train)\n\n# Create a meshgrid for the plot\nx_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\ny_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 1),\n                     np.arange(y_min, y_max, 1))\n\n# Get the predictions for each point in the meshgrid\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the prediction region\nplt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n\n# Plot the data points\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y+6, cmap='viridis')\n\n# Add a legend\nlegend = plt.legend(*scatter.legend_elements(), title='Classes')\nplt.gca().add_artist(legend)\n\n# Show the plot\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Prediction Region for Breast Cancer Dataset')\nplt.show()"
  }
]